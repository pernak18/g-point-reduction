{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By-Band _g_-Point Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "`numpy` is installed in the Python environment at NERSC (`module load python`), but `xarray` is not, so the user must install the package on their own. `PIPPATH` is the assumed location. This notebook depends heavily on `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os, sys, shutil, glob, pickle\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# \"standard\" conda install (NERSC already provides this without user \n",
    "# having to install it)\n",
    "import numpy as np\n",
    "\n",
    "# conda installs\n",
    "from tqdm import tqdm\n",
    "\n",
    "# directory in which libraries installed with conda are saved\n",
    "PIPPATH = '{}/.local/'.format(os.path.expanduser('~')) + \\\n",
    "    'cori/3.7-anaconda-2019.10/lib/python3.7/site-packages'\n",
    "PIPPATH = '/global/homes/e/emlawer/.local/cori/3.8-anaconda-2020.11/' + \\\n",
    "     'lib/python3.8/site-packages'\n",
    "PIPPATH = '/global/homes/p/pernak18/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages'\n",
    "PATHS = ['common', PIPPATH]\n",
    "for path in PATHS: sys.path.append(path)\n",
    "\n",
    "# needed at AER unless i update `pandas`\n",
    "import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# user must do `pip install xarray` on cori (or other NERSC machines)\n",
    "import xarray as xa\n",
    "\n",
    "# local modules\n",
    "import g_point_reduction as REDUX\n",
    "import modified_reduction as MODRED\n",
    "from rrtmgp_cost_compute import flux_cost_compute as FCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do one domain or the other\n",
    "DOLW = True\n",
    "DOMAIN = 'LW' if DOLW else 'SW'\n",
    "NBANDS = 16 if DOLW else 14\n",
    "\n",
    "# does band-splitting need to be done, or are there existing files \n",
    "# that have divided up the full k-distribution?\n",
    "BANDSPLIT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = '/global/project/projectdirs/e3sm/pernak18/'\n",
    "EXE = '{}/g-point-reduction/garand_atmos/rrtmgp_garand_atmos'.format(\n",
    "    PROJECT)\n",
    "REFDIR = '{}/reference_netCDF/g-point-reduce'.format(PROJECT)\n",
    "\n",
    "# test (RRTMGP) and reference (LBL) flux netCDF files, full k-distributions, \n",
    "# and by-band Garand input file\n",
    "fluxSuffix = 'flux-inputs-outputs-garandANDpreind.nc'\n",
    "if DOLW:\n",
    "    GARAND = '{}/multi_garand_template_single_band.nc'.format(REFDIR)\n",
    "    KFULLNC = '{}/rrtmgp-data-lw-g256-2018-12-04.nc'.format(REFDIR)\n",
    "    KFULLNC = '{}/rrtmgp-data-lw-g256-jen-xs.nc'.format(REFDIR)\n",
    "    REFNC = '{}/lblrtm-lw-{}'.format(REFDIR, fluxSuffix)\n",
    "    TESTNC = '{}/rrtmgp-lw-{}'.format(REFDIR, fluxSuffix)\n",
    "    #TESTNC = 'rrtmgp-lw-flux-inputs-outputs-garand-all.nc'\n",
    "else:\n",
    "    GARAND = '{}/charts_multi_garand_template_single_band.nc'.format(REFDIR)\n",
    "    KFULLNC = '{}/rrtmgp-data-sw-g224-2018-12-04.nc'.format(REFDIR)\n",
    "    REFNC = '{}/charts-sw-{}'.format(REFDIR, fluxSuffix)\n",
    "    TESTNC = '{}/rrtmgp-sw-{}'.format(REFDIR, fluxSuffix)\n",
    "# endif LW\n",
    "\n",
    "BANDSPLITDIR = 'band_k_dist'\n",
    "FULLBANDFLUXDIR = 'full_band_flux'\n",
    "\n",
    "for PATH in PATHS: FCC.pathCheck(PATH)\n",
    "\n",
    "CWD = os.getcwd()\n",
    "\n",
    "KPICKLE = '{}_k-dist.pickle'.format(DOMAIN)\n",
    "pickleCost = '{}_cost-optimize.pickle'.format(DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Band Splitting\n",
    "\n",
    "Break up full _k_-distribution file into separate distributions for each band, then calculate the corresponding fluxes. This should only need to be run once.\n",
    "\n",
    "After some clarifications from Robert (30-Nov-2020), I believe the plan of action is:\n",
    "\n",
    "1. create Nbands k-distribution files\n",
    "2. drive the Fortran executable Nbands times to produce Nbands flux results\n",
    "3. the trial g-point combinations then loop over bands and the possible g-point combinations within each band, creating k-distribution and band-wise flux files for each possible combination\n",
    "4. The Python code assembles broadband fluxes from the band-wise flux files in order to compute the cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BANDSPLIT:\n",
    "    print('Band splitting commenced')\n",
    "    FCC.pathCheck(BANDSPLITDIR, mkdir=True)\n",
    "    FCC.pathCheck(FULLBANDFLUXDIR, mkdir=True)\n",
    "    kFiles, fullBandFluxes = [], []\n",
    "    for iBand in tqdm(range(NBANDS)):\n",
    "        # divide full k-distribution into subsets for each band\n",
    "        kObj = REDUX.gCombine_kDist(KFULLNC, iBand, DOLW, 1, \n",
    "            fullBandKDir=BANDSPLITDIR, fullBandFluxDir=FULLBANDFLUXDIR, \n",
    "            profilesNC=GARAND)\n",
    "        kFiles.append(kObj.kBandNC)\n",
    "        kObj.kDistBand()\n",
    "\n",
    "        # quick, non-parallelized flux calculations (because the \n",
    "        # executable is run in one directory)\n",
    "        FCC.fluxCompute(kObj.kBandNC, kObj.profiles, kObj.exe, \n",
    "                           kObj.fullBandFluxDir, kObj.fluxBandNC)\n",
    "        fullBandFluxes.append(kObj.fluxBandNC)\n",
    "    # end band loop\n",
    "    print('Band splitting completed')\n",
    "else:\n",
    "    kFiles = sorted(glob.glob('{}/coefficients_{}_band??.nc'.format(\n",
    "        BANDSPLITDIR, DOMAIN)))\n",
    "    fullBandFluxes = sorted(glob.glob('{}/flux_{}_band??.nc'.format(\n",
    "        FULLBANDFLUXDIR, DOMAIN)))\n",
    "\n",
    "    if len(kFiles) == 0 or len(fullBandFluxes) == 0:\n",
    "        print('WARNING: set `BANDSPLIT` to `True` and run this cell again')\n",
    "# endif BANDSPLIT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pressure Levels for Cost Function\n",
    "\n",
    "Pressure levels [Pa] for the Garand atmospheres are printed to standard output with indices that can be used in the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xa.open_dataset(REFNC) as refDS:\n",
    "    pLev = refDS['p_lev'].isel(record=0)\n",
    "for iLev, pLev in enumerate(pLev.isel(col=0).values): print(iLev, pLev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _g_-Point Combining\n",
    "\n",
    "Combine _g_-point reduced for bands with full-band fluxes from other bands, find optimal _g_-point combination for given iteration, proceed to next iteration.\n",
    "\n",
    "First, find all _g_-point combinations for each band. Store the band object in a dictionary for use in flux computation. This cell only needs to be run once, and to save time in development, the dictionary is saved in a `pickle` file and can be loaded in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be parallelized; also is part of preprocessing so we \n",
    "# shouldn't have to run it multiple times\n",
    "kBandDict = {}\n",
    "for iBand, kFile in tqdm(enumerate(kFiles)):\n",
    "    #if iBand != 0: continue\n",
    "    band = iBand + 1\n",
    "    kObj = REDUX.gCombine_kDist(kFile, iBand, DOLW, 1, \n",
    "        fullBandKDir=BANDSPLITDIR, \n",
    "        fullBandFluxDir=FULLBANDFLUXDIR)\n",
    "    kObj.gPointCombine()\n",
    "    kBandDict['band{:02d}'.format(band)] = kObj\n",
    "\n",
    "    print('Band {} complete'.format(band))\n",
    "# end kFile loop\n",
    "\n",
    "import pickle\n",
    "with open(KPICKLE, 'wb') as fp: pickle.dump(kBandDict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute fluxes in parallel for every _g_-point combination -- merging occurs in each band, and these combinations in a given band are used with broadband fluxes from other bands. These concatenations each have an associated `xarray` dataset assigned to it. Cost function components are then calculated based for each dataset, and the one that minimizes the error in the cost function will have its associated netCDF saved to disk.\n",
    "\n",
    "Uncomment pickling block to restore dictionary from previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction and Optimization\n",
    "\n",
    "Test and reference netCDF files have flux and heating rate arrays of dimension `record` x `col` x `lay`/`lev` and `band` if the array is broken down by band. `record` represents atmospheric specifications that can be used in [forcing scenarios](https://github.com/pernak18/g-point-reduction/wiki/LW-Forcing-Number-Convention#g-point-reduction-convention-).\n",
    "\n",
    "Alternatively, the atmospheric specifications from any scenario can also be used. \"Bare\" parameters like `heating_rate` and `flux_net` will be treated as PD specifications, so the user will have to specify explicitly if they want the fluxes or heating rates from other scenarios by using the `flux_*_N` and `heating_rate_N` convention, where `N` is the scenario index as listed in the above list. The same convention applies to band fluxes and HRs. `N` = 0 will work just like `heating_rate` and `flux_net`.\n",
    "\n",
    "Forcing for this exercise is defined as PI subtracted from scenario (2-6). The convention for these quantities is `*_forcing_N`, where `*` is the typical flux or heating rate (band or broadband) string, and `N` again is the forcing scenario (`N` of 2 would be forcing due to doubling methane).\n",
    "\n",
    "First, let's define the cost function (component names, levels/layers, and weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling for developement purposes so this dictionary doesn't need \n",
    "# to be regenerated for every code change.\n",
    "with open(KPICKLE, 'rb') as fp: kBandDict = pickle.load(fp)\n",
    "\n",
    "# components used in cost function computation\n",
    "# variable names in RRTMGP and LBL flux netCDF file, except for \n",
    "# forcing, which has to be specifed with \"_forcing\" appended to \n",
    "# the appropriate array. e.g., \"flux_net_forcing\" for net flux forcing\n",
    "# netCDF arrays ('heating_rate', 'flux_net', 'band_flux_net', etc.)\n",
    "# or forcing scenarios: convention is  ('flux_net_forcing_3') for \n",
    "#CFCOMPS = ['flux_dif_net', 'flux_dir_dn', 'heating_rate']\n",
    "CFCOMPS = ['flux_net','band_flux_net','heating_rate','heating_rate_7',\n",
    "           'flux_net_forcing_5','flux_net_forcing_6','flux_net_forcing_7',\n",
    "           'flux_net_forcing_9','flux_net_forcing_10','flux_net_forcing_11',\n",
    "           'flux_net_forcing_12','flux_net_forcing_13','flux_net_forcing_14',\n",
    "           'flux_net_forcing_15','flux_net_forcing_16','flux_net_forcing_17',\n",
    "           'flux_net_forcing_18']\n",
    "# level indices for each component \n",
    "# (e.g., 0 for surface, 41 for Garand TOA)\n",
    "# one dictionary key per component so each component\n",
    "# can have its own set of level indices\n",
    "CFLEVS = {}\n",
    "CFLEVS['flux_net'] = [0, 26, 42]\n",
    "CFLEVS['band_flux_net'] = [42]\n",
    "CFLEVS['heating_rate'] = range(42)\n",
    "CFLEVS['heating_rate_7'] = range(42)\n",
    "CFLEVS['flux_net_forcing_5'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_6'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_7'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_9'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_10'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_11'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_12'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_13'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_14'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_15'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_16'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_17'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_18'] = [0, 26, 42]\n",
    "\n",
    "# weights for each cost function component\n",
    "CFWGT = [0.6, 0.04, 0.12, 0.12,\n",
    "         0.01, 0.02, 0.04,\n",
    "        0.005, 0.005, 0.005,\n",
    "        0.005, 0.005, 0.005, \n",
    "        0.005, 0.005, 0.005,\n",
    "        0.005]\n",
    "\n",
    "# directory under which to store k-distribution files that optimize \n",
    "# the cost function for each iteration and diagnistics (if necessary)\n",
    "CFDIR = 'fullCF_top-layer_redo_abs_parabola'\n",
    "FCC.pathCheck(CFDIR, mkdir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we starting combining _g_-points and calculating costs for all combinations, then select the trial that minimizes the cost for a given iteration. Rinse and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write diagnostic netCDFs with cost function components\n",
    "DIAGNOSTICS = True\n",
    "\n",
    "RESTORE = True\n",
    "\n",
    "if RESTORE:\n",
    "    assert os.path.exists(pickleCost), 'Cannot find {}'.format(pickleCost)\n",
    "    print('Restoring {}'.format(pickleCost))\n",
    "    with open(pickleCost, 'rb') as fp: coObj = pickle.load(fp)\n",
    "else:\n",
    "    # instantiate object for computing cost\n",
    "    coObj = REDUX.gCombine_Cost(\n",
    "        kBandDict, fullBandFluxes, REFNC, TESTNC, 1, \n",
    "        DOLW, profilesNC=GARAND, exeRRTMGP=EXE, \n",
    "        costFuncComp=CFCOMPS, costFuncLevs=CFLEVS, \n",
    "        costWeights=CFWGT, optDir='./{}'.format(CFDIR))\n",
    "# endif RESTORE\n",
    "\n",
    "# number of iterations for the optimization\n",
    "NITER = 94\n",
    "\n",
    "COSTCUT = 0.1\n",
    "\n",
    "for i in range(coObj.iCombine, NITER+1):\n",
    "    print('Iteration {}'.format(i))\n",
    "    coObj.kMap()\n",
    "    coObj.fluxComputePool()\n",
    "    coObj.fluxCombine()\n",
    "    if i == 1: coObj.costFuncComp(init=True)\n",
    "    coObj.costFuncComp()\n",
    "    coObj.findOptimal()\n",
    "    if coObj.optimized: break\n",
    "    if DIAGNOSTICS: coObj.costDiagnostics()\n",
    "\n",
    "    # Start of special g-point combination branch\n",
    "    dCostIter = np.abs(coObj.totalCost[coObj.iOpt]-coObj.winnerCost)\n",
    "    if dCostIter > COSTCUT: break\n",
    "\n",
    "    # coObj = REDUX.modCombine(coObj, i, coObj.optBand, \n",
    "    #     diagnostics=DIAGNOSTICS, \n",
    "    #     kDirIn=BANDSPLITDIR, fluxDirIn=FULLBANDFLUXDIR)\n",
    "\n",
    "    coObj.setupNextIter()\n",
    "    with open(pickleCost, 'wb') as fp: pickle.dump(coObj, fp)\n",
    "    if i in [90, 93, 100, 114]:\n",
    "        with open('LW_cost-optimize_iter{:03d}.pickle'.format(i), 'wb') as fp: pickle.dump(coObj, fp)\n",
    "    coObj.calcOptFlux(\n",
    "        fluxOutNC='optimized_{}_fluxes_iter{:03d}.nc'.format(DOMAIN, i))\n",
    "# end iteration loop\n",
    "\n",
    "KOUTNC = 'rrtmgp-data-{}-g-red.nc'.format(DOMAIN)\n",
    "\n",
    "# TO DO: getting this error:\n",
    "# `Using a DataArray object to construct a variable is ambiguous, please extract the data using the .data property.`\n",
    "# but not sure how to fix -- downgrading xarray didn't work\n",
    "# it's not vital, though, for now -- we have all the information to RESTORE then try building this \n",
    "# again when we think we have a fix, as long as the files are not deleted\n",
    "# coObj.kDistOpt(KFULLNC, kOutNC=KOUTNC)\n",
    "\n",
    "coObj.calcOptFlux(fluxOutNC='optimized_{}_fluxes.nc'.format(DOMAIN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified _g_-Point Combining\n",
    "\n",
    "At this point, we have reached large enough delta-costs -- i.e., the cost difference with respect to the **previous** iteration -- such that we would like to implement the modified _g_-point combining [described](https://github.com/pernak18/g-point-reduction/wiki/Modified-g-Point-Combining) by Eli. [Issue 19](https://github.com/pernak18/g-point-reduction/issues/19) details the refinements on this modified combination approach, namely:\n",
    "\n",
    "- parabolas are not necessary used in finding the zero crossing anymore; Eli found that the trends of scaling the weights and their associated delta-costs were approximately linear\n",
    "- we need to focus on all remaining trials, not just the \"winner\" of a given iteration as we did in Karen's implementation of the modified combining (methods with `sgl` in their names)\n",
    "\n",
    "The latter item is the stickler because we have to calculate fluxes and associated costs for all remaining trials over a few more \"weight scales\" that are the abscissa to go along with their delta-cost ordinates, which will be used in determining zero crossings and thus the scale weight that minimizes the delta-cost. Currently, we scale the _g_-point weights with `iniWgt` scaled with 0 (\"init\"), 1 (\"plus\"), and 2 (\"2plus\"), which is the naming convention used by Karen. We may revisit how many data points we use, given that a) the flux and cost computations are expensive (time), and b) delta-costs are linear with weight. \n",
    "\n",
    "First, we define some variables that are exclusive to modified combining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iniWgt = 0.05\n",
    "kModPickle = 'LW_mod-k-dist.pickle'\n",
    "costModPickle = 'LW_mod-cost-optimize.pickle'\n",
    "kBand = coObj.distBands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to apply the modified combinations and generate their corresponding _k_-distribution file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kBandDict = MODRED.kModInit(coObj.distBands, i, doLW=DOLW, \n",
    "    weight=iniWgt, kDir=BANDSPLITDIR, fluxDir=FULLBANDFLUXDIR)\n",
    "\n",
    "with open(kModPickle, 'wb') as fp: pickle.dump(kBandDict, fp)\n",
    "print('Wrote modified k-dist objects to {}'.format(kModPickle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we continue with those _k_-distributions in flux and cost computation, which is done exactly like we did with the original combinations. Note for diagnostics, we need to sync the modified cost optimization object (`coObjMod`) with original cost-optimization object (`coObj`) up to the current iteration where modification is first applied. Then we save the delta-costs for all trials in all weight scales to be used in the zero-crossing calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCostMod, coObjMod = MODRED.costModInit(coObj, kBandDict, diagnostics=True)\n",
    "with open(costModPickle, 'wb') as fp: pickle.dump(coObjMod, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newScales, cross = MODRED.scaleWeightRegress(dCostMod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another round of _k_-distribution modifications to create the netCDFs with the zero-crossing scale weights that were found in the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(costModPickle, 'rb') as fp: coObjMod = pickle.load(fp)\n",
    "coObjMod = MODRED.whereRecompute(kBand, coObjMod, cross, newScales, weight=iniWgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coObjRep = MODRED.recompute(coObj, coObjMod, np.where(cross)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to consolidate the reprocessed costs with modified-but-not-reprocessed (i.e., no zero crossing) trials. Then we can find the optimal solution after the full suite of _g_-point modification has been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coObjMod = MODRED.modOptimal(coObjMod, coObjRep, np.where(cross)[0], coObj.winnerCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save modified object WITH reprocessed results\n",
    "costRepPickle = costModPickle.replace('mod', 'rep')\n",
    "with open(costRepPickle, 'wb') as fp: pickle.dump(coObjMod, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that one iteration with modified combining is complete, we can proceed similarly to more iterations. This has the potentially to be **very** time consuming, even parallelized on a supercomputer (because of I/O? Python parallelization overhead? unsure why...), so from here on out, we'll try to use the _k_-distribution and cost optimization objects we've already taken the time to generate and only replace trials for the band that contained the winner in the previous iteration. While we're doing this only with modified combining, there is no reason this logic cannot be applied to iterations where the modified combining is not applied.\n",
    "\n",
    "Spitballing how to do this:\n",
    "- `coObjMod` trial lists -- determine indices of winner band, `costFuncComp` on only newly modified trials in said band (should only be one batch), then insert results into appropriate `coObjMod` lists at appropriate indices\n",
    "- need a `setupNextIter` analog that doesn't reset everything (`setupNextIterMod`)\n",
    "- `fluxInputsAll` might be the most important thing to alter -- that decreases in size by 1 and we need to find all of the guys with `optBand` as their `bandID`; then `costComp0`, `dCostComps0`, `dCost0`, `cost0`, `winnerCost`, `dCost`, `totalCost`, and `costComps` have to be setup, but differently than in `setupNextIter` because we don't wanna reset and rather perserve most of what's in them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on the number of _g_-points that should exist at this point: e.g., at iteration 94, after combination modification, band 12 has a winner in it. Band 12 has not had a winner since iteration 78. At that iteration, 8 _g_-points were left in the band. That number perpetuates at iteration 94 because we have not done any additional combining of points -- we've only _changed_ how the 8 points were combined (with weights and scales to the weights). I'm not sure how this will affect future iterations where a new set of combinations is generated...I believe we'll have _x_ trials that need to be recomputed, but only _x_-1 combinations, so i'll just have to use `pop` or `totalCost`, `dCost`, etc. filters for NaNs to calibrate the trial numbers for the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: verify *setupNextIter() (did we preserve what we wanted and add what we wanted?)\n",
    "# possible verification code snippet?\n",
    "# for i, fia in enumerate(coObj.fluxInputsAll):\n",
    "#     print(i, fia['bandID'], coObjMod.optBand, coObj.totalCost[i], coObj.dCost[i], coObjMod.totalCost[i], coObjMod.dCost[i], len(coObjMod.fluxInputsAll[i].keys()))\n",
    "\n",
    "# also compare initial modified trial costs to subsequent winners for verification\n",
    "# metric = np.abs(coObjMod.totalCost-coObj.winnerCost)\n",
    "# iSort = np.argsort(metric)\n",
    "# for iisort, isort in enumerate(iSort): print(isort, coObjMod.fluxInputsAll[isort]['bandID']+1, metric[isort])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as DCP\n",
    "\n",
    "with open(costModPickle, 'rb') as fp: coObjMod = pickle.load(fp)\n",
    "MODRED.modOptimal(coObjMod, coObjRep, np.where(cross)[0], coObj.winnerCost)\n",
    "\n",
    "# kFiles will likely just contain plus and 2plus (no init), because we're always doing \n",
    "# the modified combination at this point\n",
    "kFiles = {}\n",
    "coObjMod.iCombine = coObj.iCombine\n",
    "for sWgt in ['plus', '2plus']:\n",
    "    objModCP = DCP(coObjMod)\n",
    "    bandStr = 'band{:02d}'.format(coObjMod.optBand+1)\n",
    "    print(bandStr, sWgt)\n",
    "    kBandDict[sWgt][bandStr], kFiles[sWgt] = MODRED.kModSetupNextIter(\n",
    "      objModCP, iniWgt, scaleWeight=sWgt)\n",
    "    coObjMod.distBands[bandStr] = kBandDict[sWgt][bandStr]\n",
    "# end scaleWeight loop\n",
    "\n",
    "coObj0 = DCP(coObjMod)\n",
    "coObjNew = MODRED.coModSetupNextIter(coObjMod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(coObjMod.iCombine, coObjMod.iCombine+2):#NITER+1\n",
    "    print('Iteration {}'.format(i))\n",
    "\n",
    "    iBandTrials, bandObj = MODRED.doBandTrials(coObjNew, kFiles)\n",
    "    coObjNew = MODRED.modOptimal(coObjNew, bandObj, iBandTrials, coObjMod.winnerCost)\n",
    "\n",
    "    # kFiles = {}\n",
    "    # coObjNew.iCombine = coObjMod.iCombine\n",
    "    # for sWgt in ['plus', '2plus']:\n",
    "    #     objModCP = DCP(coObjMod)\n",
    "    #     bandStr = 'band{:02d}'.format(coObjMod.optBand+1)\n",
    "    #     print(bandStr, sWgt)\n",
    "    #     kBandDict[sWgt][bandStr], kFiles[sWgt] = MODRED.kModSetupNextIter(\n",
    "    #       objModCP, iniWgt, scaleWeight=sWgt)\n",
    "    #     coObjMod.distBands[bandStr] = kBandDict[sWgt][bandStr]\n",
    "    # # end scaleWeight loop\n",
    "    break\n",
    "\n",
    "    coObjNew = MODRED.coModSetupNextIter(coObjMod)\n",
    "# end combination loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point, cost0 and dCost0 for coObjMod (mid-iter 95) are of length 93 an 92\n",
    "# whereas coJbj at iter 94 (full-iter) is 94 and 93, so i forgot to append somewhere, i think\n",
    "# then i have to append the winners\n",
    "\n",
    "# for comp, cost0 in coObjMod.cost0.items():\n",
    "#     print(comp)\n",
    "#     print(len(coObjMod.cost0[comp]), len(coObjMod.dCost0[comp]))\n",
    "#     # print(len(bandObj.costComps[comp]))\n",
    "#     # print(len(bandObj.dCostComps[comp]))\n",
    "#     # print(len(bandObj.costComp0[comp]))\n",
    "#     # print(len(bandObj.dCostComps0[comp]))\n",
    "# # end comp loop\n",
    "\n",
    "# also have to replace the winner cost:\n",
    "# print(coObj.winnerCost, coObjMod.winnerCost)\n",
    "\n",
    "# when those are fixed, next step should be:\n",
    "# MODRED.modOptimal(coObjMod, bandObj, iBandTrials, coObjMod.winnerCost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
