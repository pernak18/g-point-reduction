{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By-Band _g_-Point Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "`numpy` is installed in the Python environment at NERSC (`module load python`), but `xarray` is not, so the user must install the package on their own. `PIPPATH` is the assumed location. This notebook depends heavily on `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os, sys, shutil, glob, pickle\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# \"standard\" conda install (NERSC already provides this without user \n",
    "# having to install it)\n",
    "import numpy as np\n",
    "\n",
    "# conda installs\n",
    "from tqdm import tqdm\n",
    "\n",
    "# directory in which libraries installed with conda are saved\n",
    "PIPPATH = '{}/.local/'.format(os.path.expanduser('~')) + \\\n",
    "    'cori/3.7-anaconda-2019.10/lib/python3.7/site-packages'\n",
    "PIPPATH = '/global/homes/e/emlawer/.local/cori/3.8-anaconda-2020.11/' + \\\n",
    "     'lib/python3.8/site-packages'\n",
    "PIPPATH = '/global/homes/p/pernak18/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages'\n",
    "PATHS = ['common', PIPPATH]\n",
    "for path in PATHS: sys.path.append(path)\n",
    "\n",
    "# needed at AER unless i update `pandas`\n",
    "import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# user must do `pip install xarray` on cori (or other NERSC machines)\n",
    "import xarray as xa\n",
    "\n",
    "# local modules\n",
    "import g_point_reduction as REDUX\n",
    "from rrtmgp_cost_compute import flux_cost_compute as FCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only do one domain or the other\n",
    "DOLW = True\n",
    "DOMAIN = 'LW' if DOLW else 'SW'\n",
    "NBANDS = 16 if DOLW else 14\n",
    "\n",
    "# does band-splitting need to be done, or are there existing files \n",
    "# that have divided up the full k-distribution?\n",
    "BANDSPLIT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = '/global/project/projectdirs/e3sm/pernak18/'\n",
    "EXE = '{}/g-point-reduction/garand_atmos/rrtmgp_garand_atmos'.format(\n",
    "    PROJECT)\n",
    "REFDIR = '{}/reference_netCDF/g-point-reduce'.format(PROJECT)\n",
    "\n",
    "# test (RRTMGP) and reference (LBL) flux netCDF files, full k-distributions, \n",
    "# and by-band Garand input file\n",
    "fluxSuffix = 'flux-inputs-outputs-garandANDpreind.nc'\n",
    "if DOLW:\n",
    "    GARAND = '{}/multi_garand_template_single_band.nc'.format(REFDIR)\n",
    "    KFULLNC = '{}/rrtmgp-data-lw-g256-2018-12-04.nc'.format(REFDIR)\n",
    "    KFULLNC = '{}/rrtmgp-data-lw-g256-jen-xs.nc'.format(REFDIR)\n",
    "    REFNC = '{}/lblrtm-lw-{}'.format(REFDIR, fluxSuffix)\n",
    "    TESTNC = '{}/rrtmgp-lw-{}'.format(REFDIR, fluxSuffix)\n",
    "    #TESTNC = 'rrtmgp-lw-flux-inputs-outputs-garand-all.nc'\n",
    "else:\n",
    "    GARAND = '{}/charts_multi_garand_template_single_band.nc'.format(REFDIR)\n",
    "    KFULLNC = '{}/rrtmgp-data-sw-g224-2018-12-04.nc'.format(REFDIR)\n",
    "    REFNC = '{}/charts-sw-{}'.format(REFDIR, fluxSuffix)\n",
    "    TESTNC = '{}/rrtmgp-sw-{}'.format(REFDIR, fluxSuffix)\n",
    "# endif LW\n",
    "\n",
    "BANDSPLITDIR = 'band_k_dist'\n",
    "FULLBANDFLUXDIR = 'full_band_flux'\n",
    "\n",
    "for PATH in PATHS: FCC.pathCheck(PATH)\n",
    "\n",
    "CWD = os.getcwd()\n",
    "\n",
    "KPICKLE = '{}_k-dist.pickle'.format(DOMAIN)\n",
    "pickleCost = '{}_cost-optimize.pickle'.format(DOMAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Band Splitting\n",
    "\n",
    "Break up full _k_-distribution file into separate distributions for each band, then calculate the corresponding fluxes. This should only need to be run once.\n",
    "\n",
    "After some clarifications from Robert (30-Nov-2020), I believe the plan of action is:\n",
    "\n",
    "1. create Nbands k-distribution files\n",
    "2. drive the Fortran executable Nbands times to produce Nbands flux results\n",
    "3. the trial g-point combinations then loop over bands and the possible g-point combinations within each band, creating k-distribution and band-wise flux files for each possible combination\n",
    "4. The Python code assembles broadband fluxes from the band-wise flux files in order to compute the cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BANDSPLIT:\n",
    "    print('Band splitting commenced')\n",
    "    FCC.pathCheck(BANDSPLITDIR, mkdir=True)\n",
    "    FCC.pathCheck(FULLBANDFLUXDIR, mkdir=True)\n",
    "    kFiles, fullBandFluxes = [], []\n",
    "    for iBand in tqdm(range(NBANDS)):\n",
    "        # divide full k-distribution into subsets for each band\n",
    "        kObj = REDUX.gCombine_kDist(KFULLNC, iBand, DOLW, 1, \n",
    "            fullBandKDir=BANDSPLITDIR, fullBandFluxDir=FULLBANDFLUXDIR, \n",
    "            profilesNC=GARAND)\n",
    "        kFiles.append(kObj.kBandNC)\n",
    "        kObj.kDistBand()\n",
    "\n",
    "        # quick, non-parallelized flux calculations (because the \n",
    "        # executable is run in one directory)\n",
    "        FCC.fluxCompute(kObj.kBandNC, kObj.profiles, kObj.exe, \n",
    "                           kObj.fullBandFluxDir, kObj.fluxBandNC)\n",
    "        fullBandFluxes.append(kObj.fluxBandNC)\n",
    "    # end band loop\n",
    "    print('Band splitting completed')\n",
    "else:\n",
    "    kFiles = sorted(glob.glob('{}/coefficients_{}_band??.nc'.format(\n",
    "        BANDSPLITDIR, DOMAIN)))\n",
    "    fullBandFluxes = sorted(glob.glob('{}/flux_{}_band??.nc'.format(\n",
    "        FULLBANDFLUXDIR, DOMAIN)))\n",
    "\n",
    "    if len(kFiles) == 0 or len(fullBandFluxes) == 0:\n",
    "        print('WARNING: set `BANDSPLIT` to `True` and run this cell again')\n",
    "# endif BANDSPLIT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pressure Levels for Cost Function\n",
    "\n",
    "Pressure levels [Pa] for the Garand atmospheres are printed to standard output with indices that can be used in the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xa.open_dataset(REFNC) as refDS:\n",
    "    pLev = refDS['p_lev'].isel(record=0)\n",
    "for iLev, pLev in enumerate(pLev.isel(col=0).values): print(iLev, pLev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _g_-Point Combining\n",
    "\n",
    "Combine _g_-point reduced for bands with full-band fluxes from other bands, find optimal _g_-point combination for given iteration, proceed to next iteration.\n",
    "\n",
    "First, find all _g_-point combinations for each band. Store the band object in a dictionary for use in flux computation. This cell only needs to be run once, and to save time in development, the dictionary is saved in a `pickle` file and can be loaded in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be parallelized; also is part of preprocessing so we \n",
    "# shouldn't have to run it multiple times\n",
    "kBandDict = {}\n",
    "for iBand, kFile in tqdm(enumerate(kFiles)):\n",
    "    #if iBand != 0: continue\n",
    "    band = iBand + 1\n",
    "    kObj = REDUX.gCombine_kDist(kFile, iBand, DOLW, 1, \n",
    "        fullBandKDir=BANDSPLITDIR, \n",
    "        fullBandFluxDir=FULLBANDFLUXDIR)\n",
    "    kObj.gPointCombine()\n",
    "    kBandDict['band{:02d}'.format(band)] = kObj\n",
    "\n",
    "    print('Band {} complete'.format(band))\n",
    "# end kFile loop\n",
    "\n",
    "import pickle\n",
    "with open(KPICKLE, 'wb') as fp: pickle.dump(kBandDict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute fluxes in parallel for every _g_-point combination -- merging occurs in each band, and these combinations in a given band are used with broadband fluxes from other bands. These concatenations each have an associated `xarray` dataset assigned to it. Cost function components are then calculated based for each dataset, and the one that minimizes the error in the cost function will have its associated netCDF saved to disk.\n",
    "\n",
    "Uncomment pickling block to restore dictionary from previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction and Optimization\n",
    "\n",
    "Test and reference netCDF files have flux and heating rate arrays of dimension `record` x `col` x `lay`/`lev` and `band` if the array is broken down by band. `record` represents atmospheric specifications that can be used in [forcing scenarios](https://github.com/pernak18/g-point-reduction/wiki/LW-Forcing-Number-Convention#g-point-reduction-convention-).\n",
    "\n",
    "Alternatively, the atmospheric specifications from any scenario can also be used. \"Bare\" parameters like `heating_rate` and `flux_net` will be treated as PD specifications, so the user will have to specify explicitly if they want the fluxes or heating rates from other scenarios by using the `flux_*_N` and `heating_rate_N` convention, where `N` is the scenario index as listed in the above list. The same convention applies to band fluxes and HRs. `N` = 0 will work just like `heating_rate` and `flux_net`.\n",
    "\n",
    "Forcing for this exercise is defined as PI subtracted from scenario (2-6). The convention for these quantities is `*_forcing_N`, where `*` is the typical flux or heating rate (band or broadband) string, and `N` again is the forcing scenario (`N` of 2 would be forcing due to doubling methane).\n",
    "\n",
    "First, let's define the cost function (component names, levels/layers, and weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling for developement purposes so this dictionary doesn't need \n",
    "# to be regenerated for every code change.\n",
    "with open(KPICKLE, 'rb') as fp: kBandDict = pickle.load(fp)\n",
    "\n",
    "# components used in cost function computation\n",
    "# variable names in RRTMGP and LBL flux netCDF file, except for \n",
    "# forcing, which has to be specifed with \"_forcing\" appended to \n",
    "# the appropriate array. e.g., \"flux_net_forcing\" for net flux forcing\n",
    "# netCDF arrays ('heating_rate', 'flux_net', 'band_flux_net', etc.)\n",
    "# or forcing scenarios: convention is  ('flux_net_forcing_3') for \n",
    "#CFCOMPS = ['flux_dif_net', 'flux_dir_dn', 'heating_rate']\n",
    "CFCOMPS = ['flux_net','band_flux_net','heating_rate','heating_rate_7',\n",
    "           'flux_net_forcing_5','flux_net_forcing_6','flux_net_forcing_7',\n",
    "           'flux_net_forcing_9','flux_net_forcing_10','flux_net_forcing_11',\n",
    "           'flux_net_forcing_12','flux_net_forcing_13','flux_net_forcing_14',\n",
    "           'flux_net_forcing_15','flux_net_forcing_16','flux_net_forcing_17',\n",
    "           'flux_net_forcing_18']\n",
    "# level indices for each component \n",
    "# (e.g., 0 for surface, 41 for Garand TOA)\n",
    "# one dictionary key per component so each component\n",
    "# can have its own set of level indices\n",
    "CFLEVS = {}\n",
    "CFLEVS['flux_net'] = [0, 26, 42]\n",
    "CFLEVS['band_flux_net'] = [42]\n",
    "CFLEVS['heating_rate'] = range(42)\n",
    "CFLEVS['heating_rate_7'] = range(42)\n",
    "CFLEVS['flux_net_forcing_5'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_6'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_7'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_9'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_10'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_11'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_12'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_13'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_14'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_15'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_16'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_17'] = [0, 26, 42]\n",
    "CFLEVS['flux_net_forcing_18'] = [0, 26, 42]\n",
    "\n",
    "# weights for each cost function component\n",
    "CFWGT = [0.6, 0.04, 0.12, 0.12,\n",
    "         0.01, 0.02, 0.04,\n",
    "        0.005, 0.005, 0.005,\n",
    "        0.005, 0.005, 0.005, \n",
    "        0.005, 0.005, 0.005,\n",
    "        0.005]\n",
    "\n",
    "# directory under which to store k-distribution files that optimize \n",
    "# the cost function for each iteration and diagnistics (if necessary)\n",
    "CFDIR = 'fullCF_top-layer_redo_abs_parabola'\n",
    "FCC.pathCheck(CFDIR, mkdir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we starting combining _g_-points and calculating costs for all combinations, then select the trial that minimizes the cost for a given iteration. Rinse and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write diagnostic netCDFs with cost function components\n",
    "DIAGNOSTICS = True\n",
    "\n",
    "RESTORE = True\n",
    "\n",
    "if RESTORE:\n",
    "    assert os.path.exists(pickleCost), 'Cannot find {}'.format(pickleCost)\n",
    "    print('Restoring {}'.format(pickleCost))\n",
    "    with open(pickleCost, 'rb') as fp: coObj = pickle.load(fp)\n",
    "else:\n",
    "    # instantiate object for computing cost\n",
    "    coObj = REDUX.gCombine_Cost(\n",
    "        kBandDict, fullBandFluxes, REFNC, TESTNC, 1, \n",
    "        DOLW, profilesNC=GARAND, exeRRTMGP=EXE, \n",
    "        costFuncComp=CFCOMPS, costFuncLevs=CFLEVS, \n",
    "        costWeights=CFWGT, optDir='./{}'.format(CFDIR))\n",
    "# endif RESTORE\n",
    "\n",
    "# number of iterations for the optimization\n",
    "NITER = 94\n",
    "\n",
    "COSTCUT = 0.1\n",
    "\n",
    "for i in range(coObj.iCombine, NITER+1):\n",
    "    print('Iteration {}'.format(i))\n",
    "    coObj.kMap()\n",
    "    coObj.fluxComputePool()\n",
    "    coObj.fluxCombine()\n",
    "    if i == 1: coObj.costFuncComp(init=True)\n",
    "    coObj.costFuncComp()\n",
    "    coObj.findOptimal()\n",
    "    if coObj.optimized: break\n",
    "    if DIAGNOSTICS: coObj.costDiagnostics()\n",
    "\n",
    "    # Start of special g-point combination branch\n",
    "    dCostIter = np.abs(coObj.totalCost[coObj.iOpt]-coObj.winnerCost)\n",
    "    if dCostIter > COSTCUT: break\n",
    "\n",
    "    # coObj = REDUX.modCombine(coObj, i, coObj.optBand, \n",
    "    #     diagnostics=DIAGNOSTICS, \n",
    "    #     kDirIn=BANDSPLITDIR, fluxDirIn=FULLBANDFLUXDIR)\n",
    "\n",
    "    coObj.setupNextIter()\n",
    "    with open(pickleCost, 'wb') as fp: pickle.dump(coObj, fp)\n",
    "    if i in [90, 93, 100, 114]:\n",
    "        with open('LW_cost-optimize_iter{:03d}.pickle'.format(i), 'wb') as fp: pickle.dump(coObj, fp)\n",
    "    coObj.calcOptFlux(\n",
    "        fluxOutNC='optimized_{}_fluxes_iter{:03d}.nc'.format(DOMAIN, i))\n",
    "# end iteration loop\n",
    "\n",
    "KOUTNC = 'rrtmgp-data-{}-g-red.nc'.format(DOMAIN)\n",
    "\n",
    "# TO DO: getting this error:\n",
    "# `Using a DataArray object to construct a variable is ambiguous, please extract the data using the .data property.`\n",
    "# but not sure how to fix -- downgrading xarray didn't work\n",
    "# it's not vital, though, for now -- we have all the information to RESTORE then try building this \n",
    "# again when we think we have a fix, as long as the files are not deleted\n",
    "# coObj.kDistOpt(KFULLNC, kOutNC=KOUTNC)\n",
    "\n",
    "coObj.calcOptFlux(fluxOutNC='optimized_{}_fluxes.nc'.format(DOMAIN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified _g_-Point Combining\n",
    "\n",
    "At this point, we have reached large enough delta-costs -- i.e., the cost difference with respect to the **previous** iteration -- such that we would like to implement the modified _g_-point combining [described](https://github.com/pernak18/g-point-reduction/wiki/Modified-g-Point-Combining) by Eli. [Issue 19](https://github.com/pernak18/g-point-reduction/issues/19) details the refinements on this modified combination approach, namely:\n",
    "\n",
    "- parabolas are not necessary used in finding the zero crossing anymore; Eli found that the trends of scaling the weights and their associated delta-costs were approximately linear\n",
    "- we need to focus on all remaining trials, not just the \"winner\" of a given iteration as we did in Karen's implementation of the modified combining (methods with `sgl` in their names)\n",
    "\n",
    "The latter item is the stickler because we have to calculate fluxes and associated costs for all remaining trials over a few more \"weight scales\" that are the abscissa to go along with their delta-cost ordinates, which will be used in determining zero crossings and thus the scale weight that minimizes the delta-cost. Currently, we scale the _g_-point weights with `iniWgt` scaled with 0 (\"init\"), 1 (\"plus\"), and 2 (\"2plus\"), which is the naming convention used by Karen. We may revisit how many data points we use, given that a) the flux and cost computations are expensive (time), and b) delta-costs are linear with weight. \n",
    "\n",
    "First, we have to apply the modified combinations and generate their corresponding _k_-distribution file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(coObj.fluxInputsAll[0].keys())\n",
    "# print(coObj.fluxInputsAll[-1]['bandID'])\n",
    "# print(coObj.optBand, coObj.iOpt, coObj.optNC)\n",
    "# print(coObj.fluxInputsAll[0]['fluxNC'])\n",
    "\n",
    "# gPointCombineSglPair scales initial weights with either \n",
    "# a factor of 1 or 2\n",
    "iniWgt = 0.05\n",
    "scaleWeight = ['plus', '2plus']\n",
    "\n",
    "kBand = coObj.distBands\n",
    "\n",
    "kBandDict = {}\n",
    "kBandDict['init'] = dict(kBand)\n",
    "\n",
    "for sWgt in scaleWeight:\n",
    "    print(sWgt)\n",
    "    # replace existing kBandDict with modified objects\n",
    "    kBandDict[sWgt] = {}\n",
    "    for iBand, key in enumerate(kBand.keys()):\n",
    "        band = iBand+1\n",
    "        # print('{}-Modified k for band {:d}'.format(sWgt, band))\n",
    "        kNC = kBand[key].kInNC\n",
    "        kObjMod = REDUX.gCombine_kDist(kNC, iBand, DOLW, i, \n",
    "            fullBandKDir=BANDSPLITDIR, fullBandFluxDir=FULLBANDFLUXDIR)\n",
    "\n",
    "        # how many combinations exist for band?\n",
    "        with xa.open_dataset(kNC) as ds: nCombine = ds.dims['gpt']-1\n",
    "\n",
    "        # g-point indices, and its combination \"partner\"\n",
    "        gCombine = [[x, x+1] for x in range(nCombine)]\n",
    "\n",
    "        # generate the k-dist netCDF for each combination in band\n",
    "        for comb in gCombine:\n",
    "            kObjMod.gPointCombineSglPair(sWgt, [comb], iniWgt)\n",
    "        # end gCombine loop\n",
    "\n",
    "        kBandDict[sWgt]['band{:02d}'.format(band)] = kObjMod\n",
    "    # end kBand loop\n",
    "# end scaleWeight loop\n",
    "\n",
    "outPickle = KPICKLE.replace('k-dist', 'mod-k-dist')\n",
    "with open(outPickle, 'wb') as fp: \n",
    "    pickle.dump(kBandDict, fp)\n",
    "\n",
    "print('Wrote k-dist (modified reduction) objects to {}'.format(outPickle))\n",
    "# TO DO: remove trial NC for scaleWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we continue with those _k_-distributions in flux and cost computation, which is done exactly like we did with the original combinations. Note for diagnostics, we need to sync the modified cost optimization object (`coObjMod`) with original cost-optimization object (`coObj`) up to the current iteration where modification is first applied. Then we save the delta-costs for all trials in all weight scales to be used in the zero-crossing calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCostMod = {}\n",
    "dCostMod['init'] = coObj.totalCost - coObj.winnerCost\n",
    "\n",
    "for sWgt in scaleWeight:\n",
    "    coObjMod = REDUX.gCombine_Cost(\n",
    "        kBandDict[sWgt], fullBandFluxes, REFNC, TESTNC, i, \n",
    "        DOLW, profilesNC=GARAND, exeRRTMGP=EXE, \n",
    "        costFuncComp=CFCOMPS, costFuncLevs=CFLEVS, \n",
    "        costWeights=CFWGT, optDir='./{}'.format(CFDIR))\n",
    "\n",
    "    coObjMod.kMap()\n",
    "    coObjMod.fluxComputePool()\n",
    "    coObjMod.fluxCombine()\n",
    "\n",
    "    # use coObj init costs so we don't have to do that calculation again\n",
    "    coObjMod.optNC = coObj.optNC\n",
    "    coObjMod.costComp0 = coObj.costComp0\n",
    "    coObjMod.iOpt = coObj.iOpt\n",
    "\n",
    "    # grab components up until current, modified iteration\n",
    "    for comp, cost0 in coObj.cost0.items():\n",
    "        coObjMod.cost0[comp] = cost0[:-1]\n",
    "        coObjMod.dCost0[comp] = coObj.dCost0[comp][:-1]\n",
    "        coObjMod.costComps[comp] = coObj.costComps[comp][:-1]\n",
    "        coObjMod.dCostComps[comp] = coObj.dCostComps[comp][:-1]\n",
    "        coObjMod.costComp0[comp] = coObj.costComp0[comp]\n",
    "        coObjMod.dCostComps0[comp] = coObj.dCostComps0[comp]\n",
    "    # end comp loop\n",
    "\n",
    "    # proceed with subsequent cost optimization methods\n",
    "    coObjMod.costFuncComp()\n",
    "    coObjMod.costDiagnostics()\n",
    "    dCostMod[sWgt] = coObjMod.totalCost - coObj.winnerCost\n",
    "# end scaleWeight loop\n",
    "\n",
    "pickleCost = pickleCost.replace('cost-optimize', 'mod-cost-optimize')\n",
    "with open(pickleCost, 'wb') as fp: pickle.dump(coObjMod, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit line for zero-crossing estimation\n",
    "abscissa = np.array([0, 1, 2])\n",
    "ordinates = np.vstack([dCost for dCost in dCostMod.values()])\n",
    "coeffs = np.polyfit(abscissa, ordinates, 1)\n",
    "\n",
    "# for noCross, we keep whatever scale weight produced (positive) minimum\n",
    "# and do not need to re-run flux/cost calcs\n",
    "# new scale factors for weights will start as just the scale that \n",
    "# minimized dCost, then we fill in where a zero crossing was found\n",
    "iMinDelta = np.argmin(ordinates, axis=0)\n",
    "yMin = np.nanmin(ordinates, axis=0)\n",
    "xMin = (abscissa[iMinDelta])\n",
    "y0 = ordinates[0, :]\n",
    "\n",
    "# was there a crossing at a given trial? minimum must be negative\n",
    "cross = yMin <= 0\n",
    "noCross = yMin > 0\n",
    "\n",
    "newScales = np.array(iMinDelta).astype(float)\n",
    "newScales[cross] = (xMin - xMin * yMin / (yMin - y0))[cross]\n",
    "iNan = np.where(np.isnan(newScales))\n",
    "y0[iNan] = ordinates[2, iNan]\n",
    "newScales[iNan] = (xMin - xMin * yMin / (yMin - y0))[iNan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another round of _k_-distribution modifications to create the netCDFs with the zero-crossing scale weights that were found in the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as PL\n",
    "\n",
    "gCombineAll = []\n",
    "for iBand, key in enumerate(kBand.keys()):\n",
    "    band = iBand+1\n",
    "    kNC = kBand[key].kInNC\n",
    "\n",
    "    # how many combinations exist for band?\n",
    "    with xa.open_dataset(kNC) as ds: nCombine = ds.dims['gpt']-1\n",
    "\n",
    "    # g-point indices, and its combination \"partner\"\n",
    "    gCombineAll += [[x, x+1] for x in range(nCombine)]\n",
    "# end band loop\n",
    "\n",
    "# restore modified cost-optimation object (plus/2plus scaling)\n",
    "with open(pickleCost, 'rb') as fp: coObjMod = pickle.load(fp)\n",
    "\n",
    "# loop over all trials with a zero crossing\n",
    "iReprocess = np.where(cross)[0]\n",
    "for iRep in tqdm(iReprocess):\n",
    "    # grab k-distribution for each band BEFORE g-point combining for this iteration\n",
    "    fluxInputs = coObjMod.fluxInputsAll[iRep]\n",
    "    kNC = coObj.distBands['band{:02d}'.format(fluxInputs['bandID']+1)].kInNC\n",
    "\n",
    "    g1, g2 = gCombineAll[iRep]\n",
    "\n",
    "    # generate modified k-dist netCDF file with new scale weight\n",
    "    kObjMod = REDUX.gCombine_kDist(kNC, fluxInputs['bandID'], \n",
    "        DOLW, i, fullBandKDir=BANDSPLITDIR, fullBandFluxDir=FULLBANDFLUXDIR)\n",
    "    kObjMod.gPointCombineSglPair('regress', [[g1, g2]], newScales[iRep]*iniWgt)\n",
    "\n",
    "    # replace flux computations i/o with modified files\n",
    "    coObjMod.fluxInputsAll[iRep]['kNC'] = fluxInputs['kNC'].replace('2plus', 'regress')\n",
    "    coObjMod.fluxInputsAll[iRep]['fluxNC'] = fluxInputs['fluxNC'].replace('2plus', 'regress')\n",
    "    coObjMod.fluxInputsAll[iRep]['fluxDir'] = str(fluxInputs['fluxDir']).replace('2plus', 'regress')\n",
    "# end reprocessing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy as DCP\n",
    "coObjRep = DCP(coObjMod)\n",
    "\n",
    "# reprocessing should only be done over trials with a zero crossing\n",
    "varsReduced = ['fluxInputsAll', 'trialNC', 'combinedNC', 'dCost', 'totalCost']\n",
    "iZero = np.where(cross)[0]\n",
    "for vr in varsReduced: setattr(coObjRep, vr, list(np.array(getattr(coObjRep, vr))[iZero]))\n",
    "\n",
    "coObjRep.fluxComputePool()\n",
    "coObjRep.fluxCombine()\n",
    "coObjRep.costFuncComp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab components up until current, modified iteration\n",
    "coObjRep.totalCost = []\n",
    "coObjRep.dCost = []\n",
    "coObjRep.winnerCost = float(coObj.winnerCost)\n",
    "for comp, cost0 in coObj.cost0.items():\n",
    "    coObjRep.cost0[comp] = cost0[:-1]\n",
    "    coObjRep.dCost0[comp] = coObj.dCost0[comp][:-1]\n",
    "    coObjRep.costComps[comp] = coObj.costComps[comp][:-1]\n",
    "    coObjRep.dCostComps[comp] = coObj.dCostComps[comp][:-1]\n",
    "    coObjRep.costComp0[comp] = coObj.costComp0[comp]\n",
    "    coObjRep.dCostComps0[comp] = coObj.dCostComps0[comp]\n",
    "# end comp loop\n",
    "\n",
    "coObjRep.costFuncComp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to consolidate the reprocessed costs with modified-but-not-reprocessed (i.e., no zero crossing) trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coObjMod.costFuncComp()\n",
    "with open(pickleCost, 'rb') as fp: coObjMod = pickle.load(fp)\n",
    "\n",
    "coObjMod.winnerCost = float(coObj.winnerCost)\n",
    "for iRep, iMod in enumerate(iReprocess): \n",
    "\n",
    "    coObjMod.totalCost[iMod] = float(coObjRep.totalCost[iRep])\n",
    "    for comp, cost0 in coObjRep.cost0.items():\n",
    "        coObjMod.costComps[comp][iMod] = coObjRep.costComps[comp][iRep]\n",
    "        coObjMod.dCostComps[comp][iMod] = coObjRep.dCostComps[comp][iRep]\n",
    "        coObjMod.costComp0[comp] = coObjRep.costComp0[comp]\n",
    "        coObjMod.dCostComps0[comp] = coObjRep.dCostComps0[comp]\n",
    "    # end comp loop\n",
    "# end reprocess loop\n",
    "\n",
    "coObjMod.findOptimal()\n",
    "coObjMod.costDiagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that one iteration with modified combining is complete, we can proceed similarly to more iterations. This has the potentially to be **very** time consuming, even parallelized on a supercomputer (because of I/O? Python parallelization overhead? unsure why...), so from here on out, we'll try to use the _k_-distribution and cost optimization objects we've already taken the time to generate and only replace trials for the band that contained the winner in the previous iteration. While we're doing this only with modified combining, there is no reason this logic cannot be applied to iterations where the modified combining is not applied.\n",
    "\n",
    "Eventually, all of the cells from modified combining will be placed into `g_point_reduction.py` and called in this notebook.\n",
    "\n",
    "Spitballing how to do this:\n",
    "- `kDistBand` retention, exploit `optBand` attribute again (we already do this in a number of places in `g_point_combination.py`\n",
    "- `coObjMod` trial lists -- determine indices of winner band, `costFuncComp` on only newly modified trials in said band (should only be one batch), then insert results into appropriate `coObjMod` lists at appropriate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
