{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RRTMGP _g_-Point Reduction With Cost Function Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "The global paths and how this notebook uses them have been designed to rely as little as possible on any developer's environment. The aim is:\n",
    "\n",
    "- use executable builds in E3SM Project Space on NERSC (this should be a viable option because everyone that logs into a NERSC machine will be using similar systems)\n",
    "- write outputs to the user's home directory (`~`)\n",
    "- import standard libraries (out-of-the-box Python and typical `pip` or `conda` installs like NumPy) from their NERSC locations\n",
    "- import non-standard libraries (for this notebook, just `xarray`) from Rick Pernak's Anaconda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard modules\n",
    "import os, sys, shutil, subprocess, time, multiprocessing\n",
    "\n",
    "# conda/pip installs; \"standard\" within NERSC JupyterHub\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "\n",
    "# path global variables\n",
    "HOME = os.path.expanduser('~')\n",
    "PROJECTDIR = '/project/projectdirs/e3sm/pernak18'\n",
    "GPTHOME = '{}/g-point-reduction'.format(PROJECTDIR)\n",
    "OPTPATH = '{}/k-distribution-opt'.format(GPTHOME)\n",
    "LIBPATHS = [OPTPATH, \n",
    "            '{}/.local/'.format(os.path.expanduser('~pernak18')) + \\\n",
    "            'cori/3.7-anaconda-2019.10/lib/python3.7/site-packages']\n",
    "for path in LIBPATHS: sys.path.append(path)\n",
    "\n",
    "# submodules from Robert Pincus/Ben Hillman\n",
    "# Menno also has functions with the same name\n",
    "# https://github.com/RobertPincus/k-distribution-opt/tree/brhillman/dev\n",
    "# these are in OPTPATH\n",
    "from combine_gpoints_fn import combine_gpoints_fn\n",
    "import cost_function as CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RRTMGP Library and Executable Builds\n",
    "\n",
    "First, we need to build the RTE+RRTMGP libraries and the executable that uses them -- `rrtmgp_garand_atmos` -- with the source code that is in the submodule directories `rte-rrtmgp` and ` k-distribution-opt`, respectively. This will be done in the top level directory of the clone for this repository, which has been defined as `GPTHOME` in the previous cell. To compile the `librrtmgp.a` and `librte.a` static libraries:\n",
    "\n",
    "```\n",
    "cd rte-rrtmgp/build\n",
    "ln -s Makefile.conf.ifort Makefile.conf\n",
    "make\n",
    "```\n",
    "\n",
    "And for the `rrtmgp_garand_atmos` executable, assuming the user has returned to the directory with this notebook in it:\n",
    "\n",
    "```\n",
    "cd k-distribution-opt/\n",
    "git checkout nersc-notebook\n",
    "export RRTMGP_ROOT=\"~/RRTMGP/g-point-reduction/rte-rrtmgp\"\n",
    "make\n",
    "```\n",
    "\n",
    "The absolute path for the executable will be used later in the notebook when the global variable `EXE` is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, functions were extracted from [Menno's `optimizer.py`](https://github.com/MennoVeerman/k-distribution-opt/blob/master/optimizer.py) and slightly modified. Currently, these are not used because we are starting with Ben's code. However, once we start incorporating Menno's cost function flexibility and diagnostic plotting, we may need his functions. The next cell contains functions that are compatible with Ben Hillman's [optimizerLW.py](https://github.com/RobertPincus/k-distribution-opt/blob/brhillman/dev/optimizeLW.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathCheck(path):\n",
    "    \"\"\"\n",
    "    Determine if file exists. If not, throw an Assertion Exception\n",
    "    \"\"\"\n",
    "\n",
    "    assert os.path.exists(path), 'Could not find {}'.format(path)\n",
    "\n",
    "def make_name_variant(iterNum, gpts):\n",
    "    \"\"\"\n",
    "    Return string of form iterNNN.GGG.GGG\n",
    "       iterNum is a scalar int, gpts is list of ints of length 2\n",
    "    \"\"\"\n",
    "\n",
    "    return('iter{0:03d}.{1[0]:03d}.{1[1]:03d}'.format(iterNum, gpts))\n",
    "\n",
    "def trial_cost_function(iterNum, start_kdist_file, gpts, wts, \n",
    "                        ref_flux_file, cf_norm, \n",
    "                        inVars=['net_flux', 'heating_rate', 'band_flux_net'], \n",
    "                        inP=[0, 10000, 102000], \n",
    "                        exe='./rrtmgp_garand_atmos'):\n",
    "    \"\"\"\n",
    "    Return the terms in the cost function (flux, heating rate, etc.) for a \n",
    "    proposed combination of two gpoints from an initial set.\n",
    "\n",
    "    Inputs\n",
    "        iterNum -- int, iteration number for documentation purposes\n",
    "        start_kdist_file -- string, absorption coefficients netCDF\n",
    "            used as an input into RRTMGP `exe`\n",
    "        gpts -- list of 2 ints, g-point numbers\n",
    "        wts -- list of 2 floats, weights associated with `gpts`\n",
    "        ref_flux_file -- string, RRTMGP-formatted netCDF with reference \n",
    "            (likely LBLRTM) fluxes and heating rates in it\n",
    "        cf_norm -- list of floats, normalization factors for each \n",
    "            cost function component\n",
    "    \n",
    "    Keywords\n",
    "        inVars -- list of strings, netCDF variables used in cost function\n",
    "        inP -- list of floats, pressure levels in Pa to be used in \n",
    "            cost function\n",
    "        exe -- string, path to RRTMGP executable for Garand atmospheres\n",
    "\n",
    "    Outputs\n",
    "        error_components -- list of floats, normalized RMS error \n",
    "            of RRTMGP fluxes with respect to LBLRTM reference fluxes\n",
    "    \"\"\"\n",
    "\n",
    "    trial_kdist_file = 'temp/coeffs/coefficients_{0}.nc'.format(\n",
    "        make_name_variant(iterNum, gpts))\n",
    "    trial_flux_file  = 'temp/fluxes/fluxes.all.{0}.nc'.format(\n",
    "        make_name_variant(iterNum, gpts))\n",
    "\n",
    "    # Make sure directories exist\n",
    "    os.makedirs(os.path.dirname(trial_kdist_file), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(trial_flux_file), exist_ok=True)\n",
    "\n",
    "    # Combine the pair of g-points specified in gpts(2) with weights wts(2)\n",
    "    combine_gpoints_fn(start_kdist_file, trial_kdist_file, gpts, wts)\n",
    "\n",
    "    # Compute fluxes; note we copy the template flux file first because the \n",
    "    # driver overwrites fluxes in the original file rather than writing a \n",
    "    # new file\n",
    "    shutil.copy2(ref_flux_file, trial_flux_file)\n",
    "    subprocess.run([exe, trial_flux_file, trial_kdist_file], check=True)\n",
    "\n",
    "    # Return the cost function components normalized by error at iteration 0\n",
    "    error_components = CF.normCost(\n",
    "        trial_flux_file, ref_flux_file, cf_norm, ncVars=inVars, levs=inP)\n",
    "    assert all([np.isfinite(e) for e in error_components])\n",
    "    return(error_components)\n",
    "\n",
    "def writeCostNC(iteration, inCost, gCombine, inVars, costID):\n",
    "    \"\"\"\n",
    "    Save terms of the cost function in a netCDF\n",
    "\n",
    "    Input\n",
    "        iteration -- int, g-point combination iteration number for \n",
    "          which cost function was calculated\n",
    "        inCost -- list of cost function arrays for each variable \n",
    "          in inVars\n",
    "        gCombine -- list of g-point combinations used; this is an\n",
    "          n-combination-element list of 2-element lists that \n",
    "          contain the two g-points that are combined\n",
    "        inVars -- string list of netCDF variable names used in \n",
    "          cost function calculation; should have the same number \n",
    "          of elements as gCombine does arrays\n",
    "        costID -- int, cost function index\n",
    "        normID -- int, error type or normalization ID\n",
    "\n",
    "    Output\n",
    "        netCDF file that follows the convention\n",
    "        `data/cost_function_terms.lw.iterIII.costCC.normNN.nc`\n",
    "            III -- 0-padded 3-digit iteration number\n",
    "            CC -- 0-padded 2-digit cost function ID\n",
    "            NN -- 0-padded 2-digit error/normalization ID\n",
    "    \"\"\"\n",
    "\n",
    "    ncFile = 'data/cost_function_terms.lw.iter' + \\\n",
    "        '{:03d}.cost{:02d}.nc'.format(iteration, costID)\n",
    "\n",
    "    # have to transpose the arrays for netCDF processing to work\n",
    "    inCostT = np.array(inCost).T\n",
    "\n",
    "    with nc.Dataset(ncFile, 'w') as dataOUT:\n",
    "        dataOUT.createDimension('pair', 2)\n",
    "        dataOUT.createDimension('combination', len(gCombine))\n",
    "\n",
    "        # loop over cost function components\n",
    "        for comp, sVar in zip(inCostT, inVars):\n",
    "            ncVar = dataOUT.createVariable(sVar, 'f4', ('combination'))\n",
    "            ncVar[:] = np.array(comp)\n",
    "\n",
    "        # weighted sum of all cost function components\n",
    "        totalCost = dataOUT.createVariable(\n",
    "            'total_cost_fn', 'f4', ('combination'))\n",
    "        totalCost[:] = [CF.total_cost(cost) for cost in inCost]\n",
    "\n",
    "        # g-point combinations used\n",
    "        gPtsOut = dataOUT.createVariable(\n",
    "            'Gpt_pair', 'f4', ('combination', 'pair'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the _k_-distribution optimization is done in the longwave or shortwave domain is specified with `f_thermal` (type `bool`). This is important for filename specifications. Forcing has not been implemented in this notebook, so it should **always** be set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THERMAL = True\n",
    "FORCING = False\n",
    "\n",
    "# FRANGE is a list of indices that represent forcing scenarios, \n",
    "# so it only depends on whether LW is selected, and there will\n",
    "# always be 6 forcing scenarios (in addition to no forcing)\n",
    "FRANGE = range(1+6*(THERMAL and FORCING))\n",
    "\n",
    "DOMAIN = 'lw' if THERMAL else 'sw'\n",
    "DOMAINL = 'longwave' if THERMAL else 'shortwave'\n",
    "\n",
    "# newest reference file provided by Robert (with `record` dimension)\n",
    "FILELBLRTM = '{}/inputs/g-point-reduce/lblrtm-{}'.format(PROJECTDIR, DOMAIN)\n",
    "FILELBLRTM += '-flux-inputs-outputs-garandANDpreind.nc'\n",
    "\n",
    "EXE = '{}/g-point-reduction/k-distribution-opt/rrtmgp_garand_atmos'.format(\n",
    "    PROJECTDIR)\n",
    "\n",
    "paths = [FILELBLRTM, EXE]\n",
    "for path in paths: pathCheck(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the executable is the [FORTRAN-based](https://github.com/RobertPincus/k-distribution-opt/blob/brhillman/dev/rrtmgp_garand_atmos.F90) `rrtmgp_garand_atmos` rather than the [C++-based](https://github.com/MennoVeerman/k-distribution-opt/blob/master/test_garand.cpp) `test_garand`. The latter was yielding `Segmentation Faults` at runtime on the NERSC `cori` machine and thus was not producing the results that were needed for the rest of the optimization. Because of these errors, we pursue the \"Hillman\" method in the rest of this notebook, while attempting to fold in the enhancements from Menno, which include cost function flexibility and diagnostics plotting. Hillman and Pincus also have more transparent cost function calculation, which we will try to extend to the Menno cost functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trials will typically differ in their cost function definition and normalization factor, both of which are defined in this cell. We define the cost function not only by what parameters for which errors are calculated, but also where in the atmosphere they are calculated. Any new cost function definitions should adhere to the convention in following cell -- an additional `elif` clause should be added to the `ICOST` conditional, and it should define what `varsCF` is. Strings that are permitted in `varsCF` are:\n",
    "\n",
    "- band_flux_dn\n",
    "- band_flux_net\n",
    "- band_flux_up\n",
    "- band_heating_rate\n",
    "- flux_dn\n",
    "- flux_net\n",
    "- flux_up\n",
    "- heating_rate\n",
    "\n",
    "Other variables (i.e., all that are in the netCDF) are available, but are likely not of interest for the cost function and may cause the optimization code to crash. While `ICOST` and `varsCF` could just be defined and replaced with every trial, doing it this way documents what was done for future reference.\n",
    "\n",
    "Each trial should have an associated pressure level array or list -- `pCF` -- that should be used with the variables and a pressure string -- `pStr` -- that is used with the output directories. Pressures need to be in Pa for consistency with RRTMGP convention, and the cost function code finds the pressures from `p_lev` (reference pressures from LBLRTM run -- Menno grabs the no-forcing scenario pressure profile for the first Garand atmosphere) that are closest to the user-provided `pCF` values, then averages over all pressure levels and Garand profiles.\n",
    "\n",
    "Currently, a limitation of the code used in this notebook is that cost function components are computed for one pressure range at a time. Variables in Menno's code each had corresponding regions because he had a different Python function for each cost function, which was difficult (but still possible) to generalize. We will come back to this if necessary.\n",
    "\n",
    "We can also use this block if we decide to do other kinds of normalization (AKA error type), but currently we are normalizing with the RRTGMP-LBLRTM RMS error before any reduction in _g_-points is performed. Any change in normalization should be documented in this notebook and in the output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable definition of the cost function type\n",
    "ICOST = 0\n",
    "\n",
    "# define netCDF variables in the optimization\n",
    "# ICOST 1:8 emulate Menno's cost functions; just starting this\n",
    "# define pTrop and split p_lev and use trop and strat for pCF\n",
    "if ICOST == 0: varsCF = ['flux_net', 'band_flux_net', 'heating_rate']\n",
    "elif ICOST == 2: varsCF = ['flux_net']\n",
    "else: print('INVALID COST FUNCTION')\n",
    "\n",
    "# where in the profile should we minimize the \n",
    "# no forcing, all reference pressure levels, and first Garand\n",
    "# probably should be more robust and extract \n",
    "# more than just the first Garand profile\n",
    "p_lev = nc.Dataset(FILELBLRTM).variables['p_lev'][0,:,0]\n",
    "\n",
    "# pressure at tropopause in Pa\n",
    "pTrop = np.exp(4.6) * 10\n",
    "\n",
    "pCF = np.where(p_lev >= pTrop)[0]\n",
    "pStr = 'troposphere'\n",
    "\n",
    "costStr = '{:s}_cost{:02d}_norm'.format(pStr, ICOST)\n",
    "\n",
    "print('Variables in cost function: {}'.format(', '.join(varsCF)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Assignments\n",
    "\n",
    "Paths depend on the parent directory paths, cost function specification, and whether we are examining the longwave or shortware domains. So if any of those parameters are changed, this cell needs to be re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save coeff and temporary flux files (originally `dpath`)\n",
    "outDatPath = '{}/RRTMGP/g-point-reduction/intermediate_files'.format(HOME)\n",
    "if not os.path.isdir(outDatPath): os.makedirs(outDatPath)\n",
    "\n",
    "# RRTMGP coefficient file\n",
    "coeffInit = 'rrtmgp-data-lw-g256-2018-12-04.nc' if THERMAL else \\\n",
    "    'rrtmgp-data-sw-g224-2018-12-04.nc'\n",
    "dirCoeffInit = '{}/rte-rrtmgp/rrtmgp/data/'.format(GPTHOME)\n",
    "pathCheck(dirCoeffInit)\n",
    "\n",
    "# Directory where new fluxes and coefficient files are stored.\n",
    "dirRes  = \"{}/results/{}.cost{:02d}\".format(outDatPath, DOMAIN, ICOST)\n",
    "if not os.path.isdir(dirRes): os.makedirs(dirRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Variables Definition, Extract Information from LBLRTM Reference Results\n",
    "\n",
    "The number of _g_-points can be extracted from the LBL netCDF as well. The weights associated with each _g_-point are the same for each band, so we effectively produce an `nGpt`x`nBnds` array, then flatten it to a 1-D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial settings.\n",
    "# Todo: generalize, read values from coefficient file\n",
    "# Number of bands\n",
    "nBnds = len(nc.Dataset(FILELBLRTM).variables['band'][:])\n",
    "\n",
    "# Number of G-points in each band.\n",
    "nGptsPerBandOrg = 16\n",
    "\n",
    "# Number of G-points\n",
    "nGpt = nBnds * nGptsPerBandOrg\n",
    "\n",
    "# optimization iterations\n",
    "nOptIt = 210\n",
    "nOptIt = 3\n",
    "\n",
    "# Band ID for each G-point\n",
    "bandID = range(1, nBnds+1)\n",
    "bandID = np.repeat(bandID, nGptsPerBandOrg)\n",
    "\n",
    "# G-point weights (same for all bands)\n",
    "# expand weights for one band to the rest of the bands with np.tile\n",
    "# so weights are an (nGpt x nBnds)-element vector\n",
    "wgtBnd = [0.1527534276, 0.1491729617, 0.1420961469, 0.1316886544, \n",
    "         0.1181945205, 0.1019300893, 0.0832767040, 0.0626720116, \n",
    "         0.0424925000, 0.0046269894, 0.0038279891, 0.0030260086, \n",
    "         0.0022199750, 0.0014140010, 0.0005330000, 0.0000750000]\n",
    "wt = np.tile(wgtBnd, nBnds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RRTMGP File Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input file\n",
    "\"\"\"\n",
    "file_rrtmgp_input = \"{}/input/rte_rrtmgp_input_{}.nc\".format(\n",
    "    dirData, DOMAIN)\n",
    "os.makedirs(os.path.dirname(file_rrtmgp_input), exist_ok=True)\n",
    "\n",
    "# Create output directory\n",
    "file_rrtmgp_output = \"{}/fluxes/rte_rrtmgp_output_{}.nc\".format(\n",
    "    dirData, DOMAIN)\n",
    "os.makedirs(os.path.dirname(file_rrtmgp_output), exist_ok=True)\n",
    "\n",
    "# copy original k-dist file\n",
    "shutil.copy2('{}/{}'.format(dirCoeffInit, coeffInit), \n",
    "             '{}/{}'.format(dirData, coeffInit))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial RRTMGP Fluxes (With Original _k_-distribution)/Reference Fluxes\n",
    "\n",
    "Stage some more files into a working directory (where the model is run over many iterations), then perform an initial run of RRTMGP over all Garand atmospheres. `returncode` of 0 means success. Be leery of other return codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dirRes)\n",
    "\n",
    "# copy k-distribution and profile information files to working dir\n",
    "# we are using Robert's LBL reference file for the latter\n",
    "# https://github.com/RobertPincus/k-distribution-opt/blob/master/lblrtm-lw-flux-inputs-outputs-garandANDpreind.nc\n",
    "coeffNC = 'coefficients_{}.nc'.format(DOMAIN)\n",
    "shutil.copy2('{}/{}'.format(dirCoeffInit, coeffInit), coeffNC)\n",
    "inNC = 'rte_rrtmgp_input-output_0.nc'\n",
    "shutil.copy2(FILELBLRTM, './{}'.format(inNC))\n",
    "\n",
    "# run Robert's version of `test_garand`\n",
    "args = [EXE, inNC, coeffNC]\n",
    "status = subprocess.run(args)\n",
    "if status.returncode != 0:\n",
    "    print('WARNING! {} did not complete.'.format(' '.join(args)))\n",
    "    print('This is likely because of a segmentation fault.')\n",
    "else:\n",
    "    print('Initial RRTMGP fluxes calculation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the RRTMGP fluxes from initial run so that they can be used to determine the normalization factor that is use in the optimization iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Staging\n",
    "file_rrtmgp_ref = '{}/fluxes/reference_kdist_fluxes.nc'.format(dirRes)\n",
    "if not os.path.isdir('{}/fluxes'.format(dirRes)):\n",
    "    os.makedirs('{}/fluxes'.format(dirRes))\n",
    "os.rename(inNC, file_rrtmgp_ref)\n",
    "\n",
    "# normalization factor\n",
    "cf_norm = CF.costFuncComp(\n",
    "    file_rrtmgp_ref, FILELBLRTM, ncVars=varsCF, levs=pCF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Optimization of Cost Function\n",
    "\n",
    "#### Menno version\n",
    "\n",
    "[Menno's Optimizer](https://github.com/MennoVeerman/k-distribution-opt/blob/master/optimizer.py) and Ben Hillman's (Ben separates into [LW](https://github.com/RobertPincus/k-distribution-opt/blob/brhillman/dev/optimizeLW.py) and [SW](https://github.com/RobertPincus/k-distribution-opt/blob/brhillman/dev/optimizeSW.py) optimizers) are pretty similar. Menno experiments with many more cost functions and plots intermediate results more often, but there seems to be some inconsistencies between the input files and perhaps executables that are used. For example, Menno uses `ncecat` to add a `record` dimension to the netCDF files, but Robert's and Ben's `rrtmgp_garand_atmos` expects it at runtime. Since we used `rrtmgp_garand_atmos`, we will use Ben's approach to optimization.\n",
    "\n",
    "#### Hillman Version\n",
    "\n",
    "Now the computationally-extensive part. First, some additional file staging is performed, most important being the gathering of the initial _k_-distribution for the first iteration. Then, a loop over a user-specified number of iterations (`nOptIt`) is executed, where the following steps are followed:\n",
    "\n",
    "1. _k_-distribution is defined\n",
    "2. determine all _g_-point and associated weight combinations for the iteration\n",
    "3. cost function for all possible _g_-point and weight pairs is calculated -- these pairs are distributed over many CPU threads; also calculate new coefficients and write them to their own _k_-distribution file\n",
    "4. the pair that minimized the errors is determined\n",
    "5. a new _k_-distribution, which was generated in step 3., is defined for the next iteration\n",
    "6. clean up of unnecessary files\n",
    "7. summary of optimization is printed to notebook\n",
    "8. modify weights according the optimal _g_-point combination, to be used in step 2. of the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still in dirRes\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# On first iteration, coefficients file is the original one\n",
    "shutil.copy2('{}/{}'.format(dirCoeffInit, coeffInit), \n",
    "             'data/{}'.format(coeffInit))\n",
    "coeffPrev = str(coeffInit)\n",
    "\n",
    "print(f'Combining g-point pairs {nOptIt} times...'); sys.stdout.flush()\n",
    "\n",
    "for iMain in range(1, nOptIt):\n",
    "    # Which coefficient file to use? \n",
    "    coeffIter = 'data/{}'.format(coeffPrev)\n",
    "    pathCheck(coeffIter)\n",
    "\n",
    "    # Create list of all adjacent g-point and weight pairs in each band\n",
    "    gpt_list = [[x, x+1] for x in range(1, nGpt) if \n",
    "                bandID[x-1] == bandID[x]]\n",
    "    wgt_list = [(wt[gpt_pair[0]-1], wt[gpt_pair[1]-1]) for \n",
    "                gpt_pair in gpt_list]\n",
    "\n",
    "    # Compute error terms for each combination of adjacent g-points pairs\n",
    "    # parallelize the calculations over all CPUs\n",
    "    # THIS NEEDS TO BE ADJUSTED FOR NERSC! cutting the total nCores in half\n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count() // 2) as pool:\n",
    "        # separate processes, each with their own arguments\n",
    "        results = [pool.apply_async(trial_cost_function, \n",
    "                                   args=(iMain, coeffIter, gpt_pair, \n",
    "                                         wgt_pair, FILELBLRTM, cf_norm, \n",
    "                                         varsCF, pCF, EXE))\n",
    "                   for gpt_pair, wgt_pair in zip(gpt_list, wgt_list)]\n",
    "        cfn_list = [r.get() for r in results]\n",
    "\n",
    "    # Greedy optimization\n",
    "    # Of all the g-point combinations in this iteration, \n",
    "    # which had the smallest error?\n",
    "    winner = np.argmin([CF.total_cost(x) for x in cfn_list])\n",
    "\n",
    "    # Set the new coefficent file for the next iteration.\n",
    "    coeffPrev = 'coefficients_{0}.nc'.format(\n",
    "        make_name_variant(iMain, gpt_list[winner]))\n",
    "    shutil.copy2('temp/coeffs/{}'.format(coeffPrev), \n",
    "                 'data/{}'.format(coeffPrev))\n",
    "\n",
    "    # Remove temporary files\n",
    "    for d in ['temp/coeffs', 'temp/fluxes']:\n",
    "        for f in os.listdir(d): os.remove(os.path.join(d, f))\n",
    "\n",
    "    g1out = int(gpt_list[winner][0])\n",
    "    g2out = int(gpt_list[winner][1])\n",
    "\n",
    "    # print to standard output the g-point combination that minimized error\n",
    "    print('For iteration {0:03d}, '.format(iMain), end='')\n",
    "    print('combining g-points {:03d} '.format(gpt_list[winner][0]), end='')\n",
    "    print('and {:03d} '.format(gpt_list[winner][1]), end='')\n",
    "    print('in band {:02d}'.format(bandID[g1out-1]))\n",
    "    print()\n",
    "\n",
    "    # summarize values that were optimized\n",
    "    for iVar, var in enumerate(varsCF):\n",
    "        print('{:7s}: {:9.8f}'.format(var, cfn_list[winner][iVar].values))\n",
    "\n",
    "    print('{:10s}: {:9.8f}'.format(\n",
    "        'Total Cost', CF.total_cost(cfn_list[winner])))\n",
    "    print('New coefficient file: {}'.format(coeffPrev))\n",
    "\n",
    "    # For the next iteration, modify the weights and the array that contains\n",
    "    # the number of G-points in each band. Next iteration will be over nGpts-1\n",
    "    wt[g1out-1] = wt[g1out-1] + wt[g2out-1]\n",
    "    wt = np.delete(wt, g2out-1)\n",
    "    bandID = np.delete(bandID, g2out-1)\n",
    "    nGpt -= 1\n",
    "\n",
    "    # write a netCDF for this iteration\n",
    "    writeCostNC(iMain, cfn_list, gpt_list, varsCF, ICOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
