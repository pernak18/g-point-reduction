{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RRTMGP _g_-Point Reduction With Cost Function Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard modules\n",
    "import os, sys, shutil, subprocess, time, multiprocessing\n",
    "\n",
    "# conda/pip installs\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "\n",
    "# conda and local module paths\n",
    "# we'll need the conda path for, say, xarray\n",
    "HOME = '/global/homes/p/pernak18'\n",
    "GPTHOME = '{}/RRTMGP/g-point-reduction'.format(HOME)\n",
    "OPTPATH = '{}/k-distribution-opt'.format(GPTHOME)\n",
    "LIBPATHS = [OPTPATH, \n",
    "            '{}/.local/'.format(HOME) + \\\n",
    "            'cori/3.7-anaconda-2019.10/lib/python3.7/site-packages']\n",
    "for path in LIBPATHS: sys.path.append(path)\n",
    "\n",
    "# submodules from Robert Pincus/Ben Hillman\n",
    "# Menno also has functions with the same name\n",
    "# https://github.com/RobertPincus/k-distribution-opt/tree/brhillman/dev\n",
    "# these are in OPTPATH\n",
    "from combine_gpoints_fn import combine_gpoints_fn\n",
    "import cost_function as CF\n",
    "\n",
    "# https://github.com/MennoVeerman/k-distribution-opt\n",
    "#from prepare_cpp_input import prepare_input\n",
    "#import ref_values as RV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RRTMGP Library and Executable Builds\n",
    "\n",
    "First, we need to build the RTE+RRTMGP libraries and the executable that uses them -- `rrtmgp_garand_atmos` -- with the source code that is in the submodule directories `rte-rrtmgp` and ` k-distribution-opt`, respectively. This will be done in the top level directory of the clone for this repository, which has been defined as `GPTHOME` in the previous cell. To compile the `librrtmgp.a` and `librte.a` static libraries:\n",
    "\n",
    "```\n",
    "cd rte-rrtmgp/build\n",
    "ln -s Makefile.conf.ifort Makefile.conf\n",
    "make\n",
    "```\n",
    "\n",
    "And for the `rrtmgp_garand_atmos` executable, assuming the user has returned to the directory with this notebook in it:\n",
    "\n",
    "```\n",
    "cd k-distribution-opt/\n",
    "git checkout nersc-notebook\n",
    "export RRTMGP_ROOT=\"~/RRTMGP/g-point-reduction/rte-rrtmgp\"\n",
    "make\n",
    "```\n",
    "\n",
    "The absolute path for the executable will be used later in the notebook when the global variable `EXE` is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, functions were extracted from [Menno's `optimizer.py`](https://github.com/MennoVeerman/k-distribution-opt/blob/master/optimizer.py) and slightly modified. Currently, these are not used because we are starting with Ben's code. However, once we start incorporating Menno's cost function flexibility and diagnostic plotting, we may need his functions. The next cell contains functions that are compatible with Ben Hillman's [optimizerLW.py](https://github.com/RobertPincus/k-distribution-opt/blob/brhillman/dev/optimizeLW.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathCheck(path):\n",
    "    \"\"\"\n",
    "    Determine if file exists. If not, throw an Assertion Exception\n",
    "    \"\"\"\n",
    "\n",
    "    assert os.path.exists(path), 'Could not find {}'.format(path)\n",
    "\n",
    "def trial_cost_function(iterNum, start_kdist_file, gpts, wts, \n",
    "                        ref_flux_file, cf_norm):\n",
    "    \"\"\"\n",
    "    Return the terms in the cost function (flux, heating rate, etc.) for a \n",
    "    proposed combination of two gpoints from an initial set. Hillman version\n",
    "    \"\"\"\n",
    "\n",
    "    trial_kdist_file = 'results/coeffs/coefficients_{0}.nc'.format(\n",
    "        make_name_variant(iterNum, gpts))\n",
    "    trial_flux_file  = 'results/fluxes/fluxes.all.{0}.nc'.format(\n",
    "        make_name_variant(iterNum, gpts))\n",
    "\n",
    "    # Make sure directories exist\n",
    "    os.makedirs(os.path.dirname(trial_kdist_file), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(trial_flux_file), exist_ok=True)\n",
    "\n",
    "    # Combine the pair of g-points specified in gpts(2) with weights wts(2)\n",
    "    combine_gpoints_fn(start_kdist_file, trial_kdist_file, gpts, wts)\n",
    "\n",
    "    # Compute fluxes; note we copy the template flux file first because the \n",
    "    # driver overwrites fluxes in the original file rather than writing a \n",
    "    # new file\n",
    "    shutil.copy2(ref_flux_file, trial_flux_file)\n",
    "    subprocess.run(\n",
    "        ['./rrtmgp_garand_atmos', trial_flux_file, trial_kdist_file], \n",
    "        check=True)\n",
    "\n",
    "    # Return the cost function components normalized by error at iteration 0\n",
    "    error_components = CF.normalized_cost_terms(\n",
    "        trial_flux_file, ref_flux_file, cf_norm)\n",
    "    assert all([np.isfinite(e) for e in error_components])\n",
    "    return(error_components)\n",
    "\n",
    "def writeCostNC(iteration, inCost, gCombine, inVars, costID, normID):\n",
    "    \"\"\"\n",
    "    Save terms of the cost function in a netCDF\n",
    "\n",
    "    Input\n",
    "        iteration -- int, g-point combination iteration number for \n",
    "          which cost function was calculated\n",
    "        inCost -- list of cost function arrays for each variable \n",
    "          in inVars\n",
    "        gCombine -- list of g-point combinations used; this is an\n",
    "          n-combination-element list of 2-element lists that \n",
    "          contain the two g-points that are combined\n",
    "        inVars -- string list of netCDF variable names used in \n",
    "          cost function calculation; should have the same number \n",
    "          of elements as gCombine does arrays\n",
    "        costID -- int, cost function index\n",
    "        normID -- int, error type or normalization ID\n",
    "\n",
    "    Output\n",
    "        netCDF file that follows the convention\n",
    "        `data/cost_function_terms.lw.iterIII.costCC.normNN.nc`\n",
    "            III -- 0-padded 3-digit iteration number\n",
    "            CC -- 0-padded 2-digit cost function ID\n",
    "            NN -- 0-padded 2-digit error/normalization ID\n",
    "    \"\"\"\n",
    "\n",
    "    ncFile = 'data/cost_function_terms.lw.iter' + \\\n",
    "        '{:03d}.cost{:02d}.norm{:02d}.nc'.format(iteration, costID, normID)\n",
    "\n",
    "    with nc.Dataset(ncFile, 'w') as dataOUT:\n",
    "        dataOUT.createDimension('pair', 2)\n",
    "        dataOUT.createDimension('combination', len(gCombine))\n",
    "\n",
    "        # loop over cost function components\n",
    "        for comp, sVar in zip(inCost, inVars):\n",
    "            ncVar = dataOUT.createVariable(sVar, 'f4', ('combination'))\n",
    "            ncVar[:] = np.array(comp)\n",
    "\n",
    "        # weighted sum of all cost function components\n",
    "        totalCost = dataOUT.createVariable('total_cost_fn', \"f4\", (\"Case\"))\n",
    "        totalCost[:] = CF.total_cost(inCost)\n",
    "\n",
    "        # g-point combinations used\n",
    "        gPtsOut = dataOUT.createVariable('Gpt_pair', 'f4', ('combine', 'pair'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will be expanded when Menno's implementation is...implemented.\n",
    "\n",
    "Currently, there are 8 different cost functions that can be minimized. The default is 5, but other options for `icost` (type `int`) are:\n",
    "  1. \n",
    "  2. \n",
    "  3. \n",
    "  4. \n",
    "  5. \n",
    "  6. \n",
    "  7. \n",
    "  8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables in cost function: sfc_flux\n"
     ]
    }
   ],
   "source": [
    "# Global variable definition of the cost function type\n",
    "ICOST = 5\n",
    "\n",
    "if ICOST == 1: \n",
    "    names = ['dn_bnd_lwr', 'up_bnd_lwr', 'nt_bnd_lw', \n",
    "             'dn_tot_lwr', 'up_tot_lwr', 'nt_tot_lwr', 'heat_lwr', \n",
    "             'dn_bnd_upr', 'up_bnd_upr', 'nt_bnd_up', \n",
    "             'dn_tot_upr', 'up_tot_upr', 'nt_tot_upr', 'heat_upr']\n",
    "elif ICOST == 2:\n",
    "    names = ['nt_bnd_lw', 'nt_tot_lwr', 'heat_lwr', \n",
    "             'nt_bnd_up', 'nt_tot_upr', 'heat_upr']\n",
    "elif ICOST == 3:\n",
    "    names = ['nt_bnd_lwr', 'nt_tot_lwr', 'heat_lwr']\n",
    "elif ICOST == 4:\n",
    "    names = ['nt_bnd_upr', 'nt_tot_upr', 'heat_upr']\n",
    "elif ICOST == 5:\n",
    "    names = ['sfc_flux']\n",
    "elif ICOST == 6:\n",
    "    names = ['heat_lwr', 'heat_upr', 'sfc_flux']\n",
    "elif ICOST == 7:\n",
    "    names = ['sfc_flux_dn', 'trp_flx_dn', 'trp_flx_up', 'toa_flx_up']\n",
    "elif ICOST == 8:\n",
    "    names = ['heat_thermo']\n",
    "else:\n",
    "    print('INVALID COST FUNCTION')\n",
    "    \n",
    "print('Variables in cost function: {}'.format(', '.join(names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define what kind of normalization `f_errtype` (type `int`) will be applied to the cost function:\n",
    "\n",
    "  0. absolute error (RMSE)\n",
    "  1. change in RMSE with respect to RMSE at iteration 0\n",
    "  2. error w.r.t. LBLRTM (normalized rmse)\n",
    "\n",
    "Whether the _k_-distribution optimization is done in the longwave or shortwave domain is specified with `f_thermal` (type `bool`). This is important for filename specifications. Forcing has not been implemented in this notebook, so it should **always** be set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERRTYPE = 2\n",
    "THERMAL = True\n",
    "FORCING = False\n",
    "FRANGE = range(1+6*(THERMAL and FORCING))\n",
    "DOMAIN = 'lw' if THERMAL else 'sw'\n",
    "DOMAINL = 'longwave' if THERMAL else 'shortwave'\n",
    "EXE = '{}/rrtmgp_garand_atmos'.format(OPTPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the executable is the [FORTRAN-based](https://github.com/RobertPincus/k-distribution-opt/blob/brhillman/dev/rrtmgp_garand_atmos.F90) `rrtmgp_garand_atmos` rather than the [C++-based](https://github.com/MennoVeerman/k-distribution-opt/blob/master/test_garand.cpp) `test_garand`. The latter was yielding `Segmentation Faults` at runtime on the NERSC `cori` machine and thus was not producing the results that were needed for the rest of the optimization. Because of these errors, we pursue the \"Hillman\" method in the rest of this notebook, while attempting to fold in the enhancements from Menno, which include cost function flexibility and diagnostics plotting. Hillman and Pincus also have more transparent cost function calculation, which we will try to extend to the Menno cost functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kDistOptPath = LIBPATHS[0] # path of k-distribution directory (originally `bpath`)\n",
    "pathCheck(kDistOptPath)\n",
    "\n",
    "# where to save coeff and temporary flux files (originally `dpath`)\n",
    "outDatPath = '{}/intermediate_files'.format(GPTHOME)\n",
    "if not os.path.isdir(outDatPath): os.makedirs(outDatPath)\n",
    "\n",
    "#file_LBLRTM = '{}/lbl_reference_{}.nc'.format(kDistOptPath, DOMAINL)\n",
    "# newest reference file provided by Robert (with `record` dimension)\n",
    "projectDir = '/project/projectdirs/e3sm/pernak18/inputs/g-point-reduce'\n",
    "file_LBLRTM = '{}/lblrtm-{}-flux-inputs-outputs-garandANDpreind.nc'.format(\n",
    "    projectDir, DOMAIN)\n",
    "\n",
    "# RRTMGP coefficient file\n",
    "coeffInit = 'rrtmgp-data-lw-g256-2018-12-04.nc' if THERMAL else \\\n",
    "    'rrtmgp-data-sw-g224-2018-12-04.nc'\n",
    "dirCoeffInit = '{}/rte-rrtmgp/rrtmgp/data/'.format(GPTHOME)\n",
    "pathCheck(dirCoeffInit)\n",
    "\n",
    "# Directory where new fluxes and coefficient files are stored.\n",
    "dirOut  = '{}/outputs_bnd2'.format(outDatPath)\n",
    "dirRes  = \"{}/results_bnd2/{}.cost{:02d}.norm{:01d}/\".format(\n",
    "    outDatPath, DOMAIN, ICOST, ERRTYPE)\n",
    "dirData = \"{}/data_bnd2/{}.cost{:02d}.norm{:01d}/\".format(\n",
    "    outDatPath, DOMAIN, ICOST, ERRTYPE)\n",
    "PATHS = [dirOut, dirRes, dirData]\n",
    "for path in PATHS:\n",
    "    if not os.path.isdir(path): os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Variables Definition, Extract Information from LBLRTM Reference Results\n",
    "\n",
    "We will use the pressures that were input into LBLRTM. `p_lev` is a (1 x 43 x 42) array, so we are extracting the entire pressure profile of the first Garand atmosphere (in descending order, so surface-to-TOA).\n",
    "\n",
    "The number of _g_-points can be extracted from the LBL netCDF as well. The weights associated with each _g_-point are the same for each band, so we effectively produce an `nGpt`x`nBnds` array, then flatten it to a 1-D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pressures (1D)\n",
    "p_lev = nc.Dataset(file_LBLRTM).variables['p_lev'][0,:,0]\n",
    "\n",
    "# Initial settings.\n",
    "# Todo: generalize, read values from coefficient file\n",
    "# Number of bands\n",
    "nBnds = len(nc.Dataset(file_LBLRTM).variables['band'][:])\n",
    "\n",
    "# Number of G-points in each band.\n",
    "nGptsPerBandOrg = 16\n",
    "\n",
    "# Number of G-points\n",
    "nGpt = nBnds * nGptsPerBandOrg\n",
    "\n",
    "# optimization iterations\n",
    "nOptIt = 210\n",
    "nOptIt = 3\n",
    "\n",
    "# variables in the optimization\n",
    "varsCF = ['F_', 'H_', 'FO_', 'S_']\n",
    "\n",
    "# Band ID for each G-point\n",
    "bandID = range(1, nBnds+1)\n",
    "bandID = np.repeat(bandID, nGptsPerBandOrg)\n",
    "\n",
    "# G-point weights (same for all bands)\n",
    "# expand weights for one band to the rest of the bands with np.tile\n",
    "# so weights are an (nGpt x nBnds)-element vector\n",
    "wgtBnd = [0.1527534276, 0.1491729617, 0.1420961469, 0.1316886544, \n",
    "         0.1181945205, 0.1019300893, 0.0832767040, 0.0626720116, \n",
    "         0.0424925000, 0.0046269894, 0.0038279891, 0.0030260086, \n",
    "         0.0022199750, 0.0014140010, 0.0005330000, 0.0000750000]\n",
    "wt = np.tile(wgtBnd, nBnds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RRTMGP File Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/global/homes/p/pernak18/RRTMGP/g-point-reduction/intermediate_files/data_bnd2/lw.cost05.norm2//rrtmgp-data-lw-g256-2018-12-04.nc'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare input file\n",
    "file_rrtmgp_input = \"{}/input/rte_rrtmgp_input_{}.nc\".format(\n",
    "    dirData, DOMAIN)\n",
    "os.makedirs(os.path.dirname(file_rrtmgp_input), exist_ok=True)\n",
    "#prepare_input(file_LBLRTM, file_rrtmgp_input)\n",
    "\n",
    "# Create output directory\n",
    "file_rrtmgp_output = \"{}/fluxes/rte_rrtmgp_output_{}.nc\".format(\n",
    "    dirData, DOMAIN)\n",
    "os.makedirs(os.path.dirname(file_rrtmgp_output), exist_ok=True)\n",
    "\n",
    "# copy original k-dist file\n",
    "shutil.copy2('{}/{}'.format(dirCoeffInit, coeffInit), \n",
    "             '{}/{}'.format(dirData, coeffInit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial RRTMGP Fluxes (With Original _k_-distribution)/Reference Fluxes\n",
    "\n",
    "Stage some more files into a working directory (where the model is run over many iterations), then perform an initial run of RRTMGP over all Garand atmospheres. `returncode` of 0 means success. Be leery of other return codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial RRTMGP fluxes calculation complete\n"
     ]
    }
   ],
   "source": [
    "os.chdir(dirRes)\n",
    "\n",
    "# copy k-distribution and profile information files to working dir\n",
    "# we are using Robert's LBL reference file for the latter\n",
    "# https://github.com/RobertPincus/k-distribution-opt/blob/master/lblrtm-lw-flux-inputs-outputs-garandANDpreind.nc\n",
    "coeffNC = 'coefficients_{}.nc'.format(DOMAIN)\n",
    "shutil.copy2('{}/{}'.format(dirData, coeffInit), coeffNC)\n",
    "inNC = 'rte_rrtmgp_input-output_0.nc'\n",
    "shutil.copy2(file_LBLRTM, './{}'.format(inNC))\n",
    "\n",
    "# run Robert's version of `test_garand`\n",
    "args = [EXE, inNC, coeffNC]\n",
    "status = subprocess.run(args)\n",
    "if status.returncode != 0:\n",
    "    print('WARNING! {} did not complete.'.format(' '.join(args)))\n",
    "    print('This is likely because of a segmentation fault.')\n",
    "else:\n",
    "    print('Initial RRTMGP fluxes calculation complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the RRTMGP fluxes from initial run so that they can be used to determine the normalization factor that is use in the optimization iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Staging\n",
    "file_rrtmgp_ref = '{}/fluxes/reference_kdist_fluxes.nc'.format(dirRes)\n",
    "if not os.path.isdir('{}/fluxes'.format(dirRes)):\n",
    "    os.makedirs('{}/fluxes'.format(dirRes))\n",
    "os.rename(inNC, file_rrtmgp_ref)\n",
    "\n",
    "# normalization factor\n",
    "cf_norm = CF.cost_function_components(file_rrtmgp_ref, file_LBLRTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Initialization for Greedy Optimization of Cost Function\n",
    "\n",
    "Need to make the calculations more transparent for Eli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hillman Version\n",
    "\n",
    "Now the computationally-extensive part. First, some additional file staging is performed, most important being the gathering of the initial _k_-distribution for the first iteration. Then, a loop over a user-specified number of iterations (`nOptIt`) is executed, where the following steps are followed:\n",
    "\n",
    "1. _k_-distribution is defined\n",
    "2. determine all _g_-point and associated weight combinations for the iteration\n",
    "3. cost function for all possible _g_-point and weight pairs is calculated -- these pairs are distributed over many CPU threads; also calculate new coefficients and write them to their own _k_-distribution file\n",
    "4. the pair that minimized the errors is determined\n",
    "5. a new _k_-distribution, which was generated in step 3., is defined for the next iteration\n",
    "6. clean up of unnecessary files\n",
    "7. summary of optimization is printed to notebook\n",
    "8. modify weights according the optimal _g_-point combination, to be used in step 2. of the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining g-point pairs 3 times...\n",
      "For iteration 001, combining g-points 104 and 105 in band 07\n",
      "\n",
      "F_     : 0.99959987\n",
      "H_     : 0.96116579\n",
      "FO_    : 0.99949145\n",
      "S_     : 1.00481772\n",
      "Total Cost: 0.98949512\n",
      "New coefficient file: coefficients_iter001.104.105.nc\n",
      "For iteration 002, combining g-points 076 and 077 in band 05\n",
      "\n",
      "F_     : 0.99747723\n",
      "H_     : 0.95867687\n",
      "FO_    : 0.99467427\n",
      "S_     : 1.00533259\n",
      "Total Cost: 0.98687761\n",
      "New coefficient file: coefficients_iter002.076.077.nc\n"
     ]
    }
   ],
   "source": [
    "# still in dirRes\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# On first iteration, coefficients file is the original one\n",
    "shutil.copy2('{}/{}'.format(dirCoeffInit, coeffInit), \n",
    "             'data/{}'.format(coeffInit))\n",
    "coeffPrev = str(coeffInit)\n",
    "\n",
    "print(f'Combining g-point pairs {nOptIt} times...'); sys.stdout.flush()\n",
    "\n",
    "for iMain in range(1, nOptIt):\n",
    "    # Which coefficient file to use? \n",
    "    coeffIter = 'data/{}'.format(coeffPrev)\n",
    "    pathCheck(coeffIter)\n",
    "\n",
    "    # Create list of all adjacent g-point and weight pairs in each band\n",
    "    gpt_list = [[x, x+1] for x in range(1, nGpt) if \n",
    "                bandID[x-1] == bandID[x]]\n",
    "    wgt_list = [(wt[gpt_pair[0]-1], wt[gpt_pair[1]-1]) for \n",
    "                gpt_pair in gpt_list]\n",
    "\n",
    "    # Compute error terms for each combination of adjacent g-points pairs\n",
    "    # parallelize the calculations over all CPUs\n",
    "    # THIS NEEDS TO BE ADJUSTED FOR NERSC! cutting the total nCores in half\n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count()/2) as pool:\n",
    "        # separate processes, each with their own arguments\n",
    "        results = [pool.apply_async(trial_cost_function, \n",
    "                                   args=(iMain, coeffIter, gpt_pair, \n",
    "                                         wgt_pair, file_LBLRTM, cf_norm))\n",
    "                   for gpt_pair, wgt_pair in zip(gpt_list, wgt_list)]\n",
    "        cfn_list = [r.get() for r in results]\n",
    "\n",
    "    # Greedy optimization\n",
    "    # Of all the g-point combinations in this iteration, \n",
    "    # which had the smallest error?\n",
    "    winner = np.argmin([CF.total_cost(x) for x in cfn_list])\n",
    "\n",
    "    # Set the new coefficent file for the next iteration.\n",
    "    coeffPrev = 'coefficients_{0}.nc'.format(\n",
    "        make_name_variant(iMain, gpt_list[winner]))\n",
    "    shutil.copy2('results/coeffs/{}'.format(coeffPrev), \n",
    "                 'data/{}'.format(coeffPrev))\n",
    "\n",
    "    # Remove temporary files\n",
    "    for d in ['results/coeffs', 'results/fluxes']:\n",
    "        for f in os.listdir(d): os.remove(os.path.join(d, f))\n",
    "\n",
    "    g1out = int(gpt_list[winner][0])\n",
    "    g2out = int(gpt_list[winner][1])\n",
    "\n",
    "    # print to standard output the g-point combination that minimized error\n",
    "    print('For iteration {0:03d}, '.format(iMain), end='')\n",
    "    print('combining g-points {:03d} '.format(gpt_list[winner][0]), end='')\n",
    "    print('and {:03d} '.format(gpt_list[winner][1]), end='')\n",
    "    print('in band {:02d}'.format(bandID[g1out-1]))\n",
    "    print()\n",
    "\n",
    "    # summarize values that were optimized\n",
    "    for iVar, var in enumerate(varsCF):\n",
    "        print('{:7s}: {:9.8f}'.format(var, cfn_list[winner][iVar].values))\n",
    "\n",
    "    print('{:10s}: {:9.8f}'.format(\n",
    "        'Total Cost', CF.total_cost(cfn_list[winner])))\n",
    "    print('New coefficient file: {}'.format(coeffPrev))\n",
    "\n",
    "    # For the next iteration, modify the weights and the array that contains\n",
    "    # the number of G-points in each band. Next iteration will be over nGpts-1\n",
    "    wt[g1out-1] = wt[g1out-1] + wt[g2out-1]\n",
    "    wt = np.delete(wt, g2out-1)\n",
    "    bandID = np.delete(bandID, g2out-1)\n",
    "    nGpt = nGpt-1\n",
    "\n",
    "    # write a netCDF for this iteration\n",
    "    writeCostNC(iMain, cfn_list, gpt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Menno version\n",
    "\n",
    "[Menno's Optimizer](https://github.com/MennoVeerman/k-distribution-opt/blob/master/optimizer.py) and Ben Hillman's (Ben separates into [LW](https://github.com/RobertPincus/k-distribution-opt/blob/brhillman/dev/optimizeLW.py) and [SW](https://github.com/RobertPincus/k-distribution-opt/blob/brhillman/dev/optimizeSW.py) optimizers) are pretty similar. Menno experiments with many more cost functions and plots intermediate results more often, but there seems to be some inconsistencies between the input files and perhaps executables that are used. For example, Menno uses `ncecat` to add a `record` dimension to the netCDF files, but Robert's and Ben's `rrtmgp_garand_atmos` expects it at runtime. Since we used `rrtmgp_garand_atmos`, we will use Ben's approach to optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
